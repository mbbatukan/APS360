{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO4OksS4R1U9"
      },
      "source": [
        "# Tutorial - Generative Recurrent Neural Networks\n",
        "\n",
        "Last time we discussed using recurrent neural networks to make predictions about sequences. In particular, we treated tweets as a **sequence** of words. Since tweets can have a variable number of words, we needed an architecture that can take variable-sized sequences as input.\n",
        "\n",
        "This time, we will use recurrent neural networks to **generate** sequences.\n",
        "Generating sequences is more involved compared to making predictions about\n",
        "sequences. However, it is a very interesting task, and many students chose\n",
        "sequence-generation tasks for their projects.\n",
        "\n",
        "Much of today's content is an adaptation of the \"Practical PyTorch\" GitHub \n",
        "repository [1].\n",
        "\n",
        "[1] https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiwPm7atR1VC"
      },
      "source": [
        "## Review\n",
        "\n",
        "In recurrent neural networks, the input sequence is broken down into tokens. We could choose whether to tokenize based on words, or based on characters. The representation of each token (GloVe or one-hot) is processed by the RNN one step at a time to update the hidden (or context) state.\n",
        "\n",
        "In a predictive RNN, the value of the hidden states  is a representation of **all the text that was processed thus far**. Similarly, in a generative RNN, The value of the hidden state will be a representation of **all the text that still needs to be generated**. We will use this hidden state to produce the sequence, one token at a time.\n",
        "\n",
        "Similar to the last tutorial we will break up the problem of generating text\n",
        "to generating one token at a time.\n",
        "\n",
        "We will do so with the help of two functions:\n",
        "\n",
        "1. We need to be able to generate the *next* token, given the current \n",
        "   hidden state. In practice, we get a probability distribution over \n",
        "   the next token, and sample from that probability distribution.\n",
        "2. We need to be able to update the hidden state somehow. To do so,\n",
        "   we need two pieces of information: the old hidden state, and the actual\n",
        "   token that was generated in the previous step. The actual token generated\n",
        "   will inform the subsequent tokens.\n",
        "\n",
        "We will repeat both functions until a special \"END OF SEQUENCE\" token is\n",
        "generated.\n",
        "\n",
        "Note that there are several tricky things that we will have to figure out.\n",
        "For example, how do we actually sample the actual token from the probability\n",
        "distribution over tokens? What would we do during training, and how might \n",
        "that be different from during testing/evaluation? We will answer those\n",
        "questions as we implement the RNN.\n",
        "\n",
        "For now, let's start with our training data.\n",
        "\n",
        "## Data: Donald Trump's Tweets from 2018\n",
        "\n",
        "The training set we use is a collection of Donald Trump's tweets from 2018.\n",
        "We will only use tweets that are 140 characters or shorter, and tweets\n",
        "that contains more than just a URL.\n",
        "Since tweets often contain creative spelling and numbers, and upper vs. lower\n",
        "case characters are read very differently, we will use a character-level RNN.\n",
        "\n",
        "To start, let us load the trump.csv file to Google Colab and provide access to the drive. The file can be obtained from Quercus."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch==1.8.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "%pip install torchtext==0.9 # Necessary to ensure we are using torxhtext version 0.9 that has access to legacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IaJULA6BNDk",
        "outputId": "6d0f9fe7-b356-42de-aa84-51e9cab371a1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (1982.2 MB)\n",
            "\u001b[K     |█████████████▌                  | 834.1 MB 1.4 MB/s eta 0:14:04tcmalloc: large alloc 1147494400 bytes == 0x38ebc000 @  0x7fe6d7219615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |█████████████████               | 1055.7 MB 1.2 MB/s eta 0:12:26tcmalloc: large alloc 1434370048 bytes == 0x7d512000 @  0x7fe6d7219615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |█████████████████████▋          | 1336.2 MB 1.3 MB/s eta 0:08:24tcmalloc: large alloc 1792966656 bytes == 0x2344000 @  0x7fe6d7219615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |███████████████████████████▎    | 1691.1 MB 1.2 MB/s eta 0:04:08tcmalloc: large alloc 2241208320 bytes == 0x6d12c000 @  0x7fe6d7219615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 1982251008 bytes == 0xf2a8e000 @  0x7fe6d72181e7 0x4a3940 0x4a39cc 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9\n",
            "tcmalloc: large alloc 2477817856 bytes == 0x168cfa000 @  0x7fe6d7219615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 5.3 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu111) (1.21.6)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.8.0+cu111 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.8.0+cu111 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.8.0+cu111 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0+cu111\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.9\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 27.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9) (4.64.0)\n",
            "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9) (1.8.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->torchtext==0.9) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9) (1.24.3)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.0\n",
            "    Uninstalling torchtext-0.13.0:\n",
            "      Successfully uninstalled torchtext-0.13.0\n",
            "Successfully installed torchtext-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUKaz67w3hCN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woTmhzIzR1VD",
        "outputId": "fb5b1e9c-e4c9-470b-dd8d-00f7703e602c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22402"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "# file location (make sure to use your file location)\n",
        "file_dir = '/content/drive/My Drive/Colab Notebooks/Lab 6 Tutorial/'\n",
        "\n",
        "tweets = list(line[0] for line in csv.reader(open(file_dir + 'trump.csv')))\n",
        "len(tweets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGEHJRNcR1VG"
      },
      "source": [
        "There are over 20000 tweets in this collection.\n",
        "Let's look at a few of them, just to get a sense of the kind of text\n",
        "we're dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhauODvnR1VH",
        "outputId": "7ffe3376-7efd-46e2-a909-e7d18cf888bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "God Bless the people of Venezuela!\n",
            "It was my honor. THANK YOU! https://t.co/1LvqbRQ1bi\n",
            "Nobody but Donald Trump will save Israel. You are wasting your time with these politicians and political clowns. Best! #SheldonAdelson\n"
          ]
        }
      ],
      "source": [
        "print(tweets[100])\n",
        "print(tweets[1000])\n",
        "print(tweets[10000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qksavJr_R1VK"
      },
      "source": [
        "## Generating One Tweet\n",
        "\n",
        "Normally, when we build a new machine learning model, we want to make sure\n",
        "that our model can overfit. To that end, we will first build a neural network\n",
        "that can generate _one_ tweet really well. We can choose any tweet (or any other text) we want. Let's choose to build an RNN that generates `tweet[100]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-RT-m4-R1VL",
        "outputId": "37364a04-e91b-4c58-c915-c632174ba1d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "God Bless the people of Venezuela!\n",
            "34\n"
          ]
        }
      ],
      "source": [
        "tweet = tweets[100]\n",
        "print(tweet)\n",
        "print(len(tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsdHRV1uR1VP"
      },
      "source": [
        "First, we will need to encode this tweet using a one-hot encoding.\n",
        "We'll build dictionary mappings\n",
        "from the character to the index of that character (a unique integer identifier),\n",
        "and from the index to the character. We'll use the same naming scheme that `torchtext`\n",
        "uses (`stoi` and `itos`).\n",
        "\n",
        "For simplicity, we'll work with a limited vocabulary containing\n",
        "just the characters in `tweet[100]`, plus two special tokens:\n",
        "\n",
        "- `<EOS>` represents \"End of String\", which we'll append to the end of our tweet.\n",
        "  Since tweets are variable-length, this is a way for the RNN to signal\n",
        "  that the entire sequence has been generated.\n",
        "- `<BOS>` represents \"Beginning of String\", which we'll prepend to the beginning of \n",
        "  our tweet. This is the first token that we will feed into the RNN.\n",
        "\n",
        "The way we use these special tokens will become more clear as we build the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "InvJaRXsR1VP"
      },
      "outputs": [],
      "source": [
        "vocab = list(set(tweet)) + [\"<BOS>\", \"<EOS>\"]\n",
        "vocab_stoi = {s: i for i, s in enumerate(vocab)}\n",
        "vocab_itos = {i: s for i, s in enumerate(vocab)}\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4i3C1qs5Rrt",
        "outputId": "d5454b5b-9e3c-463b-ca56-c11767168db0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['s', ' ', 'd', 'f', 'n', 'e', 't', 'G', 'p', 'z', 'B', 'h', 'a', 'u', '!', 'V', 'l', 'o', '<BOS>', '<EOS>']\n",
            "{'s': 0, ' ': 1, 'd': 2, 'f': 3, 'n': 4, 'e': 5, 't': 6, 'G': 7, 'p': 8, 'z': 9, 'B': 10, 'h': 11, 'a': 12, 'u': 13, '!': 14, 'V': 15, 'l': 16, 'o': 17, '<BOS>': 18, '<EOS>': 19}\n",
            "{0: 's', 1: ' ', 2: 'd', 3: 'f', 4: 'n', 5: 'e', 6: 't', 7: 'G', 8: 'p', 9: 'z', 10: 'B', 11: 'h', 12: 'a', 13: 'u', 14: '!', 15: 'V', 16: 'l', 17: 'o', 18: '<BOS>', 19: '<EOS>'}\n",
            "20\n"
          ]
        }
      ],
      "source": [
        "print(vocab)\n",
        "print(vocab_stoi)\n",
        "print(vocab_itos)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXknqF8gR1VS"
      },
      "source": [
        "Now that we have our vocabulary, we can build the PyTorch model\n",
        "for this problem.\n",
        "The actual model is not as complex as you might think. We actually\n",
        "already learned about all the components that we need. (Using and training\n",
        "the model is the hard part)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rZiQsDFjR1U_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XCl7ORcrR1VT"
      },
      "outputs": [],
      "source": [
        "class TextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
        "        super(TextGenerator, self).__init__()\n",
        "\n",
        "        # identiy matrix for generating one-hot vectors\n",
        "        self.ident = torch.eye(vocab_size)\n",
        "\n",
        "        # recurrent neural network\n",
        "        self.rnn = nn.GRU(vocab_size, hidden_size, n_layers, batch_first=True)\n",
        "\n",
        "        # a fully-connect layer that outputs a distribution over\n",
        "        # the next token, given the RNN output\n",
        "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
        "    \n",
        "    def forward(self, inp, hidden=None):\n",
        "        inp = self.ident[inp]                  # generate one-hot vectors of input\n",
        "        output, hidden = self.rnn(inp, hidden) # get the next output and hidden state\n",
        "        output = self.decoder(output)          # predict distribution over next tokens\n",
        "        return output, hidden\n",
        "\n",
        "model = TextGenerator(vocab_size, 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87pVghfCR1VV"
      },
      "source": [
        "## Training with Teacher Forcing\n",
        "\n",
        "At a very high level, we want our RNN model to have a high probability\n",
        "of generating the tweet. An RNN model generates text\n",
        "one character at a time based on the hidden state value.\n",
        "At each time step, we will check whether the model generated the\n",
        "correct character. That is, at each time step,\n",
        "we are trying to select the correct next character out of all the \n",
        "characters in our vocabulary. Recall that this problem is a multi-class\n",
        "classification problem, and we can use Cross-Entropy loss to train our\n",
        "network to become better at this type of problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OI8aoJRpR1VX"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvgI1WQ4R1VZ"
      },
      "source": [
        "However, we don't just have a single multi-class classification problem.\n",
        "Instead, we have **one classification problem per time-step** (per token)!\n",
        "So, how do we predict the first token in the sequence? \n",
        "How do we predict the second token in the sequence? \n",
        "\n",
        "To help you understand what happens durign RNN training, we'll start with\n",
        "inefficient training code that shows you what happens step-by-step. We'll\n",
        "start with computing the loss for the first token generated, then the second token,\n",
        "and so on.\n",
        "Later on, we'll switch to a simpler and more performant version of the code.\n",
        "\n",
        "So, let's start with the first classification problem: the problem of generating\n",
        "the **first** token (`tweet[0]`).\n",
        "\n",
        "To generate the first token, we'll feed the RNN network (with an initial, empty\n",
        "hidden state) the \"<BOS>\" token. Then, the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6dTQqfKR1Va",
        "outputId": "78d93960-f77c-4604-e983-b155fe1ea100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1]) <class 'torch.Tensor'>\n",
            "torch.Size([1]) <class 'torch.Tensor'>\n",
            "torch.Size([1, 1]) <class 'torch.Tensor'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0864,  0.0330,  0.0653, -0.0746, -0.0119, -0.0869, -0.0172,\n",
              "          -0.0979,  0.0294,  0.0267, -0.0064, -0.0517, -0.0216, -0.0640,\n",
              "          -0.0459,  0.0560, -0.0724, -0.0922, -0.1239,  0.0612]]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "bos_input = torch.Tensor([vocab_stoi[\"<BOS>\"]])\n",
        "print(bos_input.shape, type(bos_input))\n",
        "bos_input = bos_input.long()\n",
        "print(bos_input.shape, type(bos_input))\n",
        "bos_input = bos_input.unsqueeze(0)\n",
        "print(bos_input.shape, type(bos_input))\n",
        "output, hidden = model(bos_input, hidden=None)\n",
        "output # distribution over the first token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pLljlLGiw52",
        "outputId": "0b6eebea-6d85-4a6f-e39d-db2ef48fbbae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[18]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "bos_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVmBrFLVR1Vd"
      },
      "source": [
        "We can compute the loss using `criterion`. Since the model is untrained,\n",
        "the loss is expected to be high. (For now, we won't do anything\n",
        "with this loss, and omit the backward pass.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN_FopQYR1Ve",
        "outputId": "7545e387-00ea-481b-98df-5bfb21540114"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.0751, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "target = torch.Tensor([vocab_stoi[tweet[0]]]).long().unsqueeze(0)\n",
        "criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
        "          target.reshape(-1))             # reshape to 1D tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8M72rsz66Ar",
        "outputId": "beb2100f-6f3a-44f9-d91a-3d9896f173be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[7]])\n",
            "tensor([[[ 0.0864,  0.0330,  0.0653, -0.0746, -0.0119, -0.0869, -0.0172,\n",
            "          -0.0979,  0.0294,  0.0267, -0.0064, -0.0517, -0.0216, -0.0640,\n",
            "          -0.0459,  0.0560, -0.0724, -0.0922, -0.1239,  0.0612]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[ 0.0864,  0.0330,  0.0653, -0.0746, -0.0119, -0.0869, -0.0172, -0.0979,\n",
            "          0.0294,  0.0267, -0.0064, -0.0517, -0.0216, -0.0640, -0.0459,  0.0560,\n",
            "         -0.0724, -0.0922, -0.1239,  0.0612]], grad_fn=<ViewBackward>)\n",
            "tensor([7])\n"
          ]
        }
      ],
      "source": [
        "print(target)\n",
        "print(output)\n",
        "print(output.reshape(-1, vocab_size))\n",
        "print(target.reshape(-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr65KC4VR1Vg"
      },
      "source": [
        "Now, we need to update the hidden state and generate a prediction\n",
        "for the next token. To do so, we need to provide the current token to\n",
        "the RNN. We already said that during test time, we'll need to sample\n",
        "from the predicted probabilty over tokens that the neural network\n",
        "just generated. \n",
        "\n",
        "Right now, we can do something better: we can **use the ground-truth,\n",
        "actual target token**. This technique is called **teacher-forcing**, \n",
        "and generally speeds up training. The reason is that right now, \n",
        "since our model does not perform well, the predicted probability\n",
        "distribution is pretty far from the ground truth. So, it is very,\n",
        "very difficult for the neural network to get back on track given bad\n",
        "input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq5PfJvhR1Vh",
        "outputId": "0fe82144-fa12-4049-cf83-badae119a116"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0499,  0.0223,  0.0319, -0.0223,  0.0070, -0.0608, -0.0521,\n",
              "          -0.1293,  0.0113, -0.0102, -0.0344, -0.0561, -0.0203, -0.1048,\n",
              "          -0.0514,  0.0311, -0.0252, -0.0617, -0.1148,  0.0835]]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Use teacher-forcing: we pass in the ground truth `target`,\n",
        "# rather than using the NN predicted distribution\n",
        "output, hidden = model(target, hidden)\n",
        "output # distribution over the second token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U13rWRdzR1Vk"
      },
      "source": [
        "Similar to the first step, we can compute the loss, quantifying the\n",
        "difference between the predicted distribution and the actual next\n",
        "token. This loss can be used to adjust the weights of the neural\n",
        "network (which we are not doing yet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcrB_W1rR1Vl",
        "outputId": "8f7848cd-6913-40c1-ccf0-79a8ed76c6f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.0336, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "target = torch.Tensor([vocab_stoi[tweet[1]]]).long().unsqueeze(0)\n",
        "criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
        "          target.reshape(-1))             # reshape to 1D tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nXcTfTDR1Vn"
      },
      "source": [
        "We can continue this process of:\n",
        "\n",
        "- feeding the previous ground-truth token to the RNN,\n",
        "- obtaining the prediction distribution over the next token, and\n",
        "- computing the loss,\n",
        "\n",
        "for as many steps as there are tokens in the ground-truth tweet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPIFaN99R1Vo",
        "outputId": "6aadff73-4959-4cbe-c121-a439af2456ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 tensor([[[ 0.0409, -0.0133,  0.0268, -0.0090,  0.0048, -0.0664, -0.0378,\n",
            "          -0.1011,  0.0004, -0.0028, -0.0399, -0.0260, -0.0419, -0.0620,\n",
            "          -0.0636,  0.0104, -0.0434, -0.0901, -0.1486,  0.0677]]],\n",
            "       grad_fn=<AddBackward0>) tensor(2.9404, grad_fn=<NllLossBackward>)\n",
            "3 tensor([[[ 0.0549, -0.0159,  0.0251, -0.0630,  0.0098, -0.0398, -0.0125,\n",
            "          -0.1126, -0.0071, -0.0531, -0.0219, -0.0377, -0.0284, -0.0566,\n",
            "          -0.0407, -0.0169, -0.0441, -0.0767, -0.1402,  0.0749]]],\n",
            "       grad_fn=<AddBackward0>) tensor(2.9827, grad_fn=<NllLossBackward>)\n",
            "4 tensor([[[ 0.0416, -0.0229,  0.0243, -0.0450,  0.0093, -0.0124, -0.0390,\n",
            "          -0.1375, -0.0193, -0.0237, -0.0031, -0.0392, -0.0345, -0.0648,\n",
            "          -0.0088,  0.0077, -0.0722, -0.0652, -0.1343,  0.0538]]],\n",
            "       grad_fn=<AddBackward0>) tensor(2.9708, grad_fn=<NllLossBackward>)\n",
            "5 tensor([[[ 0.0171,  0.0002,  0.0278, -0.0523,  0.0537, -0.0610, -0.0495,\n",
            "          -0.1542,  0.0454,  0.0036, -0.0040, -0.0369, -0.0734, -0.0547,\n",
            "          -0.0272,  0.0412, -0.0720, -0.0582, -0.1444,  0.0438]]],\n",
            "       grad_fn=<AddBackward0>) tensor(3.0416, grad_fn=<NllLossBackward>)\n",
            "6 tensor([[[ 0.0467, -0.0141,  0.0257, -0.0421,  0.0075, -0.0477, -0.0514,\n",
            "          -0.1369,  0.0454, -0.0120, -0.0592, -0.0534, -0.0356, -0.0736,\n",
            "          -0.0265,  0.0178, -0.0739, -0.0437, -0.1464,  0.0662]]],\n",
            "       grad_fn=<AddBackward0>) tensor(3.0146, grad_fn=<NllLossBackward>)\n",
            "7 tensor([[[ 0.0808, -0.0054,  0.0165, -0.0985,  0.0247, -0.1056, -0.0086,\n",
            "          -0.1211,  0.0193,  0.0273, -0.0587, -0.0735, -0.0297, -0.0784,\n",
            "          -0.0470,  0.0308, -0.0502, -0.0744, -0.1556,  0.0696]]],\n",
            "       grad_fn=<AddBackward0>) tensor(2.8850, grad_fn=<NllLossBackward>)\n",
            "8 tensor([[[ 0.0355, -0.0068,  0.0081, -0.0953, -0.0163, -0.0943, -0.0359,\n",
            "          -0.1250,  0.0104, -0.0078, -0.0481, -0.0563, -0.0369, -0.0861,\n",
            "          -0.0349,  0.0372, -0.0507, -0.0616, -0.1170,  0.0866]]],\n",
            "       grad_fn=<AddBackward0>) tensor(2.9270, grad_fn=<NllLossBackward>)\n",
            "9 tensor([[[ 0.0159, -0.0038,  0.0015, -0.0966, -0.0350, -0.0855, -0.0467,\n",
            "          -0.1310,  0.0006, -0.0250, -0.0403, -0.0532, -0.0375, -0.0922,\n",
            "          -0.0302,  0.0388, -0.0479, -0.0512, -0.0946,  0.0935]]],\n",
            "       grad_fn=<AddBackward0>) tensor(2.9648, grad_fn=<NllLossBackward>)\n",
            "10 tensor([[[ 0.0280, -0.0034,  0.0101, -0.0696, -0.0094, -0.0357, -0.0594,\n",
            "          -0.1489, -0.0225, -0.0145, -0.0113, -0.0568, -0.0306, -0.0848,\n",
            "          -0.0022,  0.0401, -0.0689, -0.0492, -0.0954,  0.0715]]],\n",
            "       grad_fn=<AddBackward0>) tensor(3.0257, grad_fn=<NllLossBackward>)\n",
            "11 tensor([[[ 0.0749,  0.0027,  0.0117, -0.0288, -0.0071, -0.0830, -0.0142,\n",
            "          -0.1442,  0.0048, -0.0324, -0.0364, -0.0779, -0.0339, -0.0689,\n",
            "          -0.0620,  0.0319, -0.0545, -0.0510, -0.0997,  0.0873]]],\n",
            "       grad_fn=<AddBackward0>) tensor(3.0461, grad_fn=<NllLossBackward>)\n",
            "12 tensor([[[ 0.0826,  0.0092,  0.0287, -0.0381,  0.0148, -0.0653, -0.0029,\n",
            "          -0.1479,  0.0296, -0.0297, -0.0336, -0.0867, -0.0175, -0.0688,\n",
            "          -0.0519, -0.0044,  0.0056, -0.0591, -0.1058,  0.1133]]],\n",
            "       grad_fn=<AddBackward0>) tensor(3.0414, grad_fn=<NllLossBackward>)\n",
            "13 tensor([[[ 0.0995,  0.0118,  0.0165, -0.1011,  0.0324, -0.1061,  0.0181,\n",
            "          -0.1270,  0.0107,  0.0108, -0.0410, -0.0904, -0.0194, -0.0750,\n",
            "          -0.0589,  0.0233, -0.0058, -0.0803, -0.1305,  0.0930]]],\n",
            "       grad_fn=<AddBackward0>) tensor(2.9601, grad_fn=<NllLossBackward>)\n",
            "14 tensor([[[ 0.0663,  0.0026,  0.0198, -0.0686,  0.0238, -0.0471, -0.0286,\n",
            "          -0.1393, -0.0094, -0.0006, -0.0111, -0.0678, -0.0277, -0.0712,\n",
            "          -0.0147,  0.0357, -0.0472, -0.0675, -0.1164,  0.0728]]],\n",
            "       grad_fn=<AddBackward0>) tensor(2.9818, grad_fn=<NllLossBackward>)\n",
            "15 tensor([[[ 0.0492,  0.0210, -0.0300, -0.0427,  0.0183, -0.0454, -0.0575,\n",
            "          -0.1425,  0.0160, -0.0029, -0.0385, -0.0433, -0.0283, -0.0514,\n",
            "           0.0171,  0.0657, -0.0531, -0.0788, -0.0745,  0.0525]]],\n",
            "       grad_fn=<AddBackward0>) tensor(3.0199, grad_fn=<NllLossBackward>)\n",
            "16 tensor([[[ 0.0885,  0.0138, -0.0103, -0.1022,  0.0374, -0.0968, -0.0139,\n",
            "          -0.1298,  0.0038,  0.0340, -0.0466, -0.0664, -0.0260, -0.0648,\n",
            "          -0.0207,  0.0540, -0.0373, -0.1008, -0.1229,  0.0650]]],\n",
            "       grad_fn=<AddBackward0>) tensor(3.0714, grad_fn=<NllLossBackward>)\n",
            "17 tensor([[[ 0.0658, -0.0158,  0.0030, -0.0509,  0.0241, -0.0816, -0.0273,\n",
            "          -0.0968, -0.0028,  0.0133, -0.0439, -0.0253, -0.0447, -0.0354,\n",
            "          -0.0371,  0.0264, -0.0462, -0.1126, -0.1472,  0.0664]]],\n",
            "       grad_fn=<AddBackward0>) tensor(2.9716, grad_fn=<NllLossBackward>)\n",
            "18 tensor([[[ 0.0516,  0.0091, -0.0422, -0.0303,  0.0179, -0.0554, -0.0635,\n",
            "          -0.1232,  0.0159,  0.0037, -0.0565, -0.0131, -0.0304, -0.0304,\n",
            "           0.0145,  0.0629, -0.0516, -0.0992, -0.0906,  0.0490]]],\n",
            "       grad_fn=<AddBackward0>) tensor(3.0255, grad_fn=<NllLossBackward>)\n",
            "19 tensor([[[ 0.0723, -0.0134, -0.0087, -0.0297, -0.0073, -0.0356, -0.0631,\n",
            "          -0.1220,  0.0238, -0.0087, -0.0838, -0.0358, -0.0088, -0.0560,\n",
            "           0.0086,  0.0303, -0.0645, -0.0720, -0.1250,  0.0685]]],\n",
            "       grad_fn=<AddBackward0>) tensor(3.0062, grad_fn=<NllLossBackward>)\n",
            "20 tensor([[[ 0.0978, -0.0062, -0.0020, -0.0904,  0.0187, -0.0954, -0.0182,\n",
            "          -0.1135,  0.0040,  0.0301, -0.0707, -0.0614, -0.0128, -0.0653,\n",
            "          -0.0210,  0.0384, -0.0454, -0.0926, -0.1479,  0.0685]]],\n",
            "       grad_fn=<AddBackward0>) tensor(2.9746, grad_fn=<NllLossBackward>)\n",
            "21 tensor([[[ 0.0680, -0.0108,  0.0110, -0.0590,  0.0163, -0.0441, -0.0499,\n",
            "          -0.1321, -0.0152,  0.0127, -0.0286, -0.0523, -0.0228, -0.0639,\n",
            "           0.0097,  0.0424, -0.0694, -0.0778, -0.1301,  0.0587]]],\n",
            "       grad_fn=<AddBackward0>) tensor(3.0482, grad_fn=<NllLossBackward>)\n",
            "22 tensor([[[ 0.0572, -0.0295,  0.0130, -0.0246,  0.0131, -0.0576, -0.0379,\n",
            "          -0.1013, -0.0081,  0.0049, -0.0363, -0.0231, -0.0459, -0.0361,\n",
            "          -0.0275,  0.0143, -0.0615, -0.1023, -0.1532,  0.0582]]],\n",
            "       grad_fn=<AddBackward0>) tensor(2.9924, grad_fn=<NllLossBackward>)\n",
            "23 tensor([[[ 0.0601, -0.0003,  0.0037, -0.0540,  0.0185, -0.0610, -0.0895,\n",
            "          -0.1066,  0.0015, -0.0073, -0.0372, -0.0479, -0.0408, -0.0586,\n",
            "           0.0249,  0.0486, -0.0852, -0.0837, -0.1474,  0.0649]]],\n",
            "       grad_fn=<AddBackward0>) tensor(2.9678, grad_fn=<NllLossBackward>)\n",
            "24 tensor([[[ 0.0565, -0.0101,  0.0092, -0.0400,  0.0180, -0.0172, -0.0829,\n",
            "          -0.1401, -0.0154, -0.0012, -0.0180, -0.0474, -0.0375, -0.0646,\n",
            "           0.0283,  0.0408, -0.0889, -0.0768, -0.1326,  0.0556]]],\n",
            "       grad_fn=<AddBackward0>) tensor(2.9282, grad_fn=<NllLossBackward>)\n",
            "25 tensor([[[ 0.0887, -0.0319, -0.0071, -0.0576,  0.0049, -0.0215, -0.0704,\n",
            "          -0.1403, -0.0211, -0.0292, -0.0209, -0.0191, -0.0296, -0.0900,\n",
            "          -0.0110,  0.0773, -0.0597, -0.0811, -0.1154,  0.0738]]],\n",
            "       grad_fn=<AddBackward0>) tensor(2.9909, grad_fn=<NllLossBackward>)\n",
            "26 tensor([[[ 0.1093, -0.0135, -0.0017, -0.1096,  0.0315, -0.0776, -0.0217,\n",
            "          -0.1313, -0.0161,  0.0212, -0.0378, -0.0519, -0.0282, -0.0875,\n",
            "          -0.0353,  0.0585, -0.0424, -0.1011, -0.1442,  0.0743]]],\n",
            "       grad_fn=<AddBackward0>) tensor(2.9362, grad_fn=<NllLossBackward>)\n",
            "27 tensor([[[ 0.1080,  0.0363, -0.0064, -0.0859, -0.0023, -0.0586, -0.0260,\n",
            "          -0.1128,  0.0278,  0.0263, -0.0588, -0.0551, -0.0171, -0.0587,\n",
            "           0.0145,  0.0588, -0.0215, -0.1226, -0.1061,  0.0773]]],\n",
            "       grad_fn=<AddBackward0>) tensor(3.0371, grad_fn=<NllLossBackward>)\n",
            "28 tensor([[[ 1.1434e-01,  2.2746e-02, -2.7830e-03, -1.2118e-01,  2.8071e-02,\n",
            "          -1.0453e-01, -9.9374e-05, -1.1388e-01,  5.4206e-03,  4.7742e-02,\n",
            "          -5.9715e-02, -7.3827e-02, -2.2602e-02, -7.0601e-02, -1.6070e-02,\n",
            "           5.7714e-02, -2.4237e-02, -1.1585e-01, -1.3535e-01,  7.1748e-02]]],\n",
            "       grad_fn=<AddBackward0>) tensor(2.9248, grad_fn=<NllLossBackward>)\n",
            "29 tensor([[[ 0.0778,  0.0422, -0.0112, -0.0587,  0.0326, -0.1126, -0.0124,\n",
            "          -0.0580,  0.0324,  0.0047, -0.0457, -0.0379, -0.0351, -0.0352,\n",
            "          -0.0155,  0.0647, -0.0310, -0.0881, -0.1553,  0.0732]]],\n",
            "       grad_fn=<AddBackward0>) tensor(3.0143, grad_fn=<NllLossBackward>)\n",
            "30 tensor([[[ 0.0759,  0.0246, -0.0229, -0.0582, -0.0188, -0.0849, -0.0049,\n",
            "          -0.0678, -0.0094, -0.0018, -0.0195, -0.0561,  0.0056, -0.0480,\n",
            "          -0.0388,  0.0752, -0.0111, -0.1160, -0.1557,  0.0335]]],\n",
            "       grad_fn=<AddBackward0>) tensor(3.0573, grad_fn=<NllLossBackward>)\n",
            "31 tensor([[[ 0.0981,  0.0098, -0.0021, -0.1043,  0.0117, -0.1240,  0.0107,\n",
            "          -0.0863, -0.0088,  0.0323, -0.0335, -0.0724, -0.0077, -0.0633,\n",
            "          -0.0529,  0.0680, -0.0203, -0.1135, -0.1657,  0.0447]]],\n",
            "       grad_fn=<AddBackward0>) tensor(2.9892, grad_fn=<NllLossBackward>)\n",
            "32 tensor([[[ 0.0877, -0.0103,  0.0140, -0.0683, -0.0148, -0.0818, -0.0284,\n",
            "          -0.0921,  0.0111, -0.0021, -0.0726, -0.0667, -0.0004, -0.0745,\n",
            "          -0.0278,  0.0436, -0.0492, -0.0730, -0.1582,  0.0667]]],\n",
            "       grad_fn=<AddBackward0>) tensor(2.9679, grad_fn=<NllLossBackward>)\n",
            "33 tensor([[[ 0.0757, -0.0317,  0.0304, -0.0085, -0.0034, -0.0979, -0.0411,\n",
            "          -0.0678, -0.0114, -0.0314, -0.0542, -0.0540, -0.0358, -0.0734,\n",
            "          -0.0182,  0.0401, -0.0232, -0.0816, -0.1554,  0.0877]]],\n",
            "       grad_fn=<AddBackward0>) tensor(2.9878, grad_fn=<NllLossBackward>)\n"
          ]
        }
      ],
      "source": [
        "for i in range(2, len(tweet)):\n",
        "    output, hidden = model(target, hidden)\n",
        "    target = torch.Tensor([vocab_stoi[tweet[i]]]).long().unsqueeze(0)\n",
        "    loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
        "                     target.reshape(-1))             # reshape to 1D tensor\n",
        "    print(i, output, loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPcGN8T9R1Vr"
      },
      "source": [
        "Finally, with our final token, we should expect to output the \"<EOS>\"\n",
        "token, so that our RNN learns when to stop generating characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNpcFt7MR1Vs",
        "outputId": "4d4386b3-78db-499c-aef3-3a4ee7738500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33 tensor([[[ 0.0837, -0.0484,  0.0037, -0.0262, -0.0054, -0.0636, -0.0039,\n",
            "          -0.0972, -0.0037, -0.0724, -0.0550, -0.0971, -0.0152, -0.0912,\n",
            "          -0.0069,  0.0197, -0.0410, -0.0639, -0.1346,  0.0635]]],\n",
            "       grad_fn=<AddBackward0>) tensor(2.9009, grad_fn=<NllLossBackward>)\n"
          ]
        }
      ],
      "source": [
        "output, hidden = model(target, hidden)\n",
        "target = torch.Tensor([vocab_stoi[\"<EOS>\"]]).long().unsqueeze(0)\n",
        "loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
        "                 target.reshape(-1))             # reshape to 1D tensor\n",
        "print(i, output, loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9izimdzFR1Vv"
      },
      "source": [
        "In practice, we don't really need a loop. Recall that in a predictive RNN,\n",
        "the `nn.RNN` module can take an entire sequence as input. We can do the\n",
        "same thing here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd9MhPC-R1Vw",
        "outputId": "b8b8143b-f23b-4195-bdd4-7891ebbb50f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 36])\n"
          ]
        }
      ],
      "source": [
        "tweet_ch = [\"<BOS>\"] + list(tweet) + [\"<EOS>\"]\n",
        "tweet_indices = [vocab_stoi[ch] for ch in tweet_ch]\n",
        "tweet_tensor = torch.Tensor(tweet_indices).long().unsqueeze(0)\n",
        "\n",
        "print(tweet_tensor.shape)\n",
        "\n",
        "output, hidden = model(tweet_tensor[:,:-1]) # <EOS> is never an input token\n",
        "target = tweet_tensor[:,1:]                 # <BOS> is never a target token\n",
        "loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
        "                 target.reshape(-1))             # reshape to 1D tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWOEipAcR1Vz"
      },
      "source": [
        "Here, the input to our neural network model is the *entire*\n",
        "sequence of input tokens (everything from \"<BOS>\" to the\n",
        "last character of the tweet). The neural network generates a prediction distribution\n",
        "of the next token at each step. We can compare each of these  with the ground-truth\n",
        "`target`.\n",
        "\n",
        "\n",
        "Our training loop (for learning to generate the single `tweet`) will therefore\n",
        "look something like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnDk217g9Paf",
        "outputId": "def88a68-1c4c-4d3f-a46a-be4cd77d34e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[18,  7, 17,  2,  1, 10, 16,  5,  0,  0,  1,  6, 11,  5,  1,  8,  5, 17,\n",
            "          8, 16,  5,  1, 17,  3,  1, 15,  5,  4,  5,  9, 13,  5, 16, 12, 14]])\n",
            "tensor([[ 7, 17,  2,  1, 10, 16,  5,  0,  0,  1,  6, 11,  5,  1,  8,  5, 17,  8,\n",
            "         16,  5,  1, 17,  3,  1, 15,  5,  4,  5,  9, 13,  5, 16, 12, 14, 19]])\n"
          ]
        }
      ],
      "source": [
        "print(tweet_tensor[:,:-1])\n",
        "print(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HG6kNxDR1Vz",
        "outputId": "35f9dd92-dc63-4ea9-bc82-0c69032252f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iter 100] Loss 1.767341\n",
            "[Iter 200] Loss 0.266412\n",
            "[Iter 300] Loss 0.040046\n",
            "[Iter 400] Loss 0.016533\n",
            "[Iter 500] Loss 0.009435\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "for it in range(500):\n",
        "    optimizer.zero_grad()\n",
        "    output, _ = model(tweet_tensor[:,:-1])\n",
        "    loss = criterion(output.reshape(-1, vocab_size),\n",
        "                 target.reshape(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (it+1) % 100 == 0:\n",
        "        print(\"[Iter %d] Loss %f\" % (it+1, float(loss)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lADJ6zO0R1V2"
      },
      "source": [
        "The training loss is decreasing with training, which is what we expect.\n",
        "\n",
        "## Generating a Token\n",
        "\n",
        "At this point, we want to see whether our model is actually learning\n",
        "something. So, we need to talk about how to\n",
        "actually use the RNN model to generate text. If we can \n",
        "generate text, we can make a qualitative asssessment of how well\n",
        "our RNN is performing.\n",
        "\n",
        "The main difference between training and test-time (generation time)\n",
        "is that we don't have the ground-truth tokens to feed as inputs\n",
        "to the RNN. Instead, we need to actually **sample** a token based\n",
        "on the neural network's prediction distribution.\n",
        "\n",
        "But how can we sample a token from a distribution?\n",
        "\n",
        "On one extreme, we can always take\n",
        "the token with the largest probability (argmax). This has been our\n",
        "go-to technique in other classification tasks. However, this idea\n",
        "will fail here. The reason is that in practice, \n",
        "**we want to be able to generate a variety of different sequences from\n",
        "the same model**. An RNN that can only generate a single new Trump Tweet\n",
        "is fairly useless.\n",
        "\n",
        "In short, we want some randomness. We can do so by using the logit\n",
        "outputs from our model to construct a multinomial distribution over\n",
        "the tokens and then sample a random token from that multinomial distribution.\n",
        "\n",
        "One natural multinomial distribution we can choose is the \n",
        "distribution we get after applying the softmax on the outputs.\n",
        "However, we will do one more thing: we will add a **temperature**\n",
        "parameter to manipulate the softmax outputs. We can set a\n",
        "**higher temperature** to make the probability of each token\n",
        "**more even** (more random), or a **lower temperature** to assign\n",
        "more probability to the tokens with a higher logit (output).\n",
        "A **higher temperature** means that we will get a more diverse sample,\n",
        "with potentially more mistakes. A **lower temperature** means that we\n",
        "may see repetitions of the same high probability sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCBOf-UlR1V3",
        "outputId": "7139812a-9f08-4ae3-c372-af2228c7e9df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "God Bless the people of Venezuela!\n",
            "God Bless the people of Venezuela!\n",
            "God Blesstthh peoplefof VenezuelalaG\n",
            "GolsBless the people of VenezfeuVa\n",
            "h\n"
          ]
        }
      ],
      "source": [
        "def sample_sequence(model, max_len=100, temperature=0.8):\n",
        "    generated_sequence = \"\"\n",
        "   \n",
        "    inp = torch.Tensor([vocab_stoi[\"<BOS>\"]]).long()\n",
        "    hidden = None\n",
        "    for p in range(max_len):\n",
        "        output, hidden = model(inp.unsqueeze(0), hidden)\n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = vocab_itos[top_i]\n",
        "        \n",
        "        if predicted_char == \"<EOS>\":\n",
        "            break\n",
        "        generated_sequence += predicted_char       \n",
        "        inp = torch.Tensor([top_i]).long()\n",
        "    return generated_sequence\n",
        "\n",
        "print(sample_sequence(model, temperature=0.8))\n",
        "print(sample_sequence(model, temperature=1.0))\n",
        "print(sample_sequence(model, temperature=1.5))\n",
        "print(sample_sequence(model, temperature=2.0))\n",
        "print(sample_sequence(model, temperature=5.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61I-4wHHR1WB"
      },
      "source": [
        "Since we only trained the model on a single sequence, we won't see\n",
        "the effect of the temperature parameter yet. \n",
        "\n",
        "For now, the output of the calls to the `sample_sequence` function\n",
        "assures us that our training code looks reasonable, and we can\n",
        "proceed to training on our full dataset!\n",
        "\n",
        "## Training the Trump Tweet Generator\n",
        "\n",
        "For the actual training, let's use `torchtext` so that we can use\n",
        "the `BucketIterator` to make batches. Like in Lab 5, we'll create a \n",
        "`torchtext.legacy.data.Field` to use `torchtext` to read the CSV file, and convert\n",
        "characters into indices. The object has convenient parameters to specify\n",
        "the BOS and EOS tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TaOu21fR1WC",
        "outputId": "e29643c3-9ab6-4c65-e4d1-6e7792a45722"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22402"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "import torchtext\n",
        "\n",
        "text_field = torchtext.legacy.data.Field(sequential=True, # text sequence\n",
        "                                  tokenize=lambda x: x, # because we are building a character-RNN\n",
        "                                  include_lengths=True, # to track the length of sequences, for batching\n",
        "                                  batch_first=True,\n",
        "                                  use_vocab=True,       # to turn each character into an integer index\n",
        "                                  init_token=\"<BOS>\",   # BOS token\n",
        "                                  eos_token=\"<EOS>\")    # EOS token\n",
        "\n",
        "fields = [('text', text_field), ('created_at', None), ('id_str', None)]\n",
        "trump_tweets = torchtext.legacy.data.TabularDataset(file_dir + \"trump.csv\", \"csv\", fields)\n",
        "len(trump_tweets) # should be >20,000 like before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q-fvtXAR1WE",
        "outputId": "10c90da3-db13-498f-9441-37153905a950"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "253"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "text_field.build_vocab(trump_tweets)\n",
        "vocab_stoi = text_field.vocab.stoi # so we don't have to rewrite sample_sequence\n",
        "vocab_itos = text_field.vocab.itos # so we don't have to rewrite sample_sequence\n",
        "vocab_size = len(text_field.vocab.itos)\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fVuKmNIR1WH"
      },
      "source": [
        "Let's just verify that the `BucketIterator` works as expected, but start with batch_size of 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UL64eVUR1WI",
        "outputId": "9312217b-fea4-45aa-9782-fe1a230efc2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "tensor([113, 113, 113, 112, 112, 112, 112, 111, 111, 111])\n",
            "torch.Size([10, 113])\n"
          ]
        }
      ],
      "source": [
        "data_iter = torchtext.legacy.data.BucketIterator(trump_tweets, \n",
        "                                          batch_size=10,\n",
        "                                          sort_key=lambda x: len(x.text),\n",
        "                                          sort_within_batch=True)\n",
        "for (tweet, lengths), label in data_iter:\n",
        "    print(label)   # should be None\n",
        "    print(lengths) # contains the length of the tweet(s) in batch\n",
        "    print(tweet.shape) # should be [10, max(length)]\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pftGNL97R1WK"
      },
      "source": [
        "To account for batching, our actual training code will change, but just a little bit.\n",
        "In fact, our training code from before will work with a batch size larger than ten!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wLx8pz5mR1WL"
      },
      "outputs": [],
      "source": [
        "def train(model, data, batch_size=1, num_epochs=1, lr=0.001, print_every=100):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    it = 0\n",
        "    \n",
        "    data_iter = torchtext.legacy.data.BucketIterator(data,\n",
        "                                              batch_size=batch_size,\n",
        "                                              sort_key=lambda x: len(x.text),\n",
        "                                              sort_within_batch=True)\n",
        "    for e in range(num_epochs):\n",
        "        # get training set\n",
        "        avg_loss = 0\n",
        "        for (tweet, lengths), label in data_iter:\n",
        "            target = tweet[:, 1:] # Exclude BOS\n",
        "            inp = tweet[:, :-1] # Exclude EOS\n",
        "            # cleanup\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass\n",
        "            output, _ = model(inp)\n",
        "            loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1))\n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            avg_loss += loss\n",
        "            it += 1 # increment iteration count\n",
        "            if it % print_every == 0:\n",
        "                print(\"[Iter %d] Loss %f\" % (it+1, float(avg_loss/print_every)))\n",
        "                print(\"    \" + sample_sequence(model, 140, 0.8))\n",
        "                avg_loss = 0\n",
        "\n",
        "model = TextGenerator(vocab_size, 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUNZfzbb_0_s",
        "outputId": "cf11b4a1-e68a-4509-83d5-813f7636209f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iter 101] Loss 3.717335\n",
            "    Pl tahelheaoa-Gdo  llte  ap \n",
            "[Iter 201] Loss 3.228811\n",
            "    The veds van home aw t yd nanenea (omnin hg0 nh/ Bt amou ouwt! an @tan  en Iont onth w /ad tirolir” b.p se the anouOt Hes ont 'one @ke athe \n",
            "[Iter 301] Loss 2.986001\n",
            "    Sanlan Pzere  he tozyru ang allerone @0held yOrr rvens\"a tocaty afinnLurtB \" Tre f.s thamyten ory we:!\n",
            "[Iter 401] Loss 2.915044\n",
            "    iole't aneg on be kerbridt\n",
            "[Iter 501] Loss 2.797385\n",
            "    Gof.ce ppat. ounindo6 ist-lDonk.runf Arump itting bot. Riat ag done latt p for bes sonat me He Irat inanconn W- bat!\n",
            "[Iter 601] Loss 2.667882\n",
            "    @GA O- an ile thucind mes4 Pine is anding the cer Be real TMat. 0 AUmp Binaly fhandered distind deal onad bep hat  old Ga domad at in ere Me\n",
            "[Iter 701] Loss 2.593516\n",
            "    Doneling oon bantinn reat scourine OuNI. Trump @ru2lDoneldadPreanife wincing illicaldTrutp andere\n",
            "[Iter 801] Loss 2.524553\n",
            "    @DonalDent oner cump me to ans the reay sered real of how preat Treaj? ht.  U4F #LSNingine this  ale iling coume Sures ouk tht fo ail beas .\n",
            "[Iter 901] Loss 2.463648\n",
            "    @RericldTrump @rextalone @red_Prick ouldTrump furesiWe. The ch to falive fronkx dibly in the MA Trump You an on the che th thid thanking pbe\n",
            "[Iter 1001] Loss 2.427105\n",
            "    @merstirgardenkeving: Stepshtis.\n",
            "[Iter 1101] Loss 2.376328\n",
            "    Ami😂 CotrealDonaldTrump ang thanis to the deing and matis sore ant is anci te for onald them bis ball fullatided ap ston adeake deald. Wiald\n",
            "[Iter 1201] Loss 2.361237\n",
            "    jeaca - arling it che! https://t.co/Xg♡nYsVH4!\n",
            "[Iter 1301] Loss 2.325848\n",
            "    @MeymanHandangnokitpres pooke to iblo A are of Jutt Caterich a farell the frear your for wom seay s.\n",
            "[Iter 1401] Loss 2.304114\n",
            "    Thatd of fos a bict ard anchankin hegright thing all Thim MeGRR and uster. TharAN THRIN-TOur!\n",
            "[Iter 1501] Loss 2.288583\n",
            "    Band atate sing http://t.co/XXliJqlbl\n",
            "[Iter 1601] Loss 2.207207\n",
            "    An @realDonaldTrump @Caskebuntryers are .S........ https://t.co/lqWxUkijIgyot\n",
            "[Iter 1701] Loss 2.278297\n",
            "    Thank you thanad &amp; #Sandingeet be and gat ock soues the tha day! http://t.co/Ml8dMsQOD\n",
            "[Iter 1801] Loss 2.246164\n",
            "    Mallent fay ind Baldarp oul for aed starnerablact a gaded Cosed to Great Poedertenters arveride moven in ingret to hay prow beted wh th 400 \n",
            "[Iter 1901] Loss 2.204997\n",
            "    Vorr prost a cool rave F Thenk cove in more vop https://t.co/tiX0SYPrent #MANBILYA Thanks ou Trump Hople sesaing the I. Thank\n",
            "[Iter 2001] Loss 2.218529\n",
            "    Tat un whtton #HewuhttrealFor #CLESBARISSLOC! Bin Fornting have to 20%.\n",
            "[Iter 2101] Loss 2.159631\n",
            "    \"reat ay ApreatDonald had whhtow Sfirs dant tod het on 16Uth sore prusiass and bunt's Americadat!\n",
            "[Iter 2201] Loss 2.183654\n",
            "    AME! UC SIS is soffera lof the beald be that lay the a more for joshe on ive Nablio in will dest rofre. So of dountical  a. Anes bere concac\n",
            "[Iter 2301] Loss 2.162406\n",
            "    Heand with showe Lo MESHEASS.. https://t.co/nvAK1gKIj\n",
            "[Iter 2401] Loss 2.124669\n",
            "    ge look do gat is labsinge read aly more chenget wish the in fisitesticatsing neving a woel! Stadest.\n",
            "[Iter 2501] Loss 2.108164\n",
            "    .@Wavericallare @farnumbanilling and of @raseAmprump; gis dees tonat with at @peeppsoterick @realDonaldTrump bug in we Hinall for wat how an\n",
            "[Iter 2601] Loss 2.098948\n",
            "    The wance is at is goon Trump Thank\n",
            "[Iter 2701] Loss 2.104313\n",
            "    Alaistion just  fikeoud on the be uncrepp cuz and love mote!\n",
            "[Iter 2801] Loss 2.105282\n",
            "     and of cuntinatsing Inners Kindse it is tintrent a dine tromed semy thes padisntmay!\n",
            "[Iter 2901] Loss 2.105493\n",
            "    Thanks and  fous and a wo to sloncally to great.. It ofule tometoone for the getreperstale. #Trump2016 Bromingation!\n",
            "[Iter 3001] Loss 2.096551\n",
            "    INNo auld I've go a let now of wether the the beallase the reed de on that thill Nep doingionsh yom a det hon whe and stoug in the for is ou\n",
            "[Iter 3101] Loss 2.072281\n",
            "    I jake day first Flirald rouly ired to the seredievine douce by over bame st.  Preetss prowe forsher! #Megraice bast if Eay is are on forso \n",
            "[Iter 3201] Loss 2.088284\n",
            "    @jomer: show Obama peline Mr to ir wition las har vorre luser Great Obain wbom.\n",
            "[Iter 3301] Loss 2.080071\n",
            "    The  t uprey doys the isen! Lote the coullr by any freming. - job!\n",
            "[Iter 3401] Loss 2.013480\n",
            "    ATrump Trump is coed Me todenine? Thank you @MACazing hiving to great tod wangy. Makn excetice and tore the ind ared and our ublice goitul j\n",
            "[Iter 3501] Loss 2.056938\n",
            "    @Fockarn617: @realDonaldTrump con't he in Thonk you'T Kespert the than\n",
            "[Iter 3601] Loss 2.038697\n",
            "    The I WBighise ammettent counisers peopling and solicald. I will be the remightide. Billesifinges to gat a president so babcest hetre m. wit\n",
            "[Iter 3701] Loss 2.073629\n",
            "    @JeSND: @realDonaldTrump won President compores ank truth bedainlewse pread The!\n",
            "[Iter 3801] Loss 2.111842\n",
            "    @INA @bectedtersing @Negand http://t.co/lfWjgxfj33\n",
            "[Iter 3901] Loss 2.043529\n",
            "    Sumating the Trump Todion. The leome sige on chank is Vicr care has so this fars leamers the Mrous!\n",
            "[Iter 4001] Loss 2.002209\n",
            "    Hover will about oud masting leasen respe to Be is wathen for peopmer!\n",
            "[Iter 4101] Loss 1.958588\n",
            "    @Gaig1 @Trump we the have on @TrumpTouse ald be offrie the fiinsh reaal!\n",
            "[Iter 4201] Loss 2.026736\n",
            "    @Briidbratid: You't mey of great geat haw showel fox fill wast stay in Kotily done happors ctighting. The from Anaring\" who kiver—great on y\n",
            "[Iter 4301] Loss 2.032999\n",
            "    “Chipporth Agorge twis https://t.co/NN2wSEqjjH\n",
            "[Iter 4401] Loss 1.975848\n",
            "    Will be andinge worrest getter ‘Trumpant... on you to have was st his pordonall the enderalty!\n",
            "[Iter 4501] Loss 1.930859\n",
            "    @reallarghef: @realDonaldTrump @Mackince.” I Donal Collar the proin mand be. Whith that the Hillore Woll the knester doin to .@moolyfurlly M\n",
            "[Iter 4601] Loss 1.984193\n",
            "    In fing Donald Trump Seging” https://t.co/EEwO6ZyfJi\n",
            "[Iter 4701] Loss 1.951675\n",
            "    Land worke happe never than Proom... her prectice the ir my amen Job pain and trump the Crick1 - Loon has F they notes on it htaps sigh sure\n",
            "[Iter 4801] Loss 1.977718\n",
            "    @cheminder: @replorlagry: @realDonaldTrump @nackancotingry Good @realDonold than of bove my lias the ruck the weal @TrumpTSOGAC!\n",
            "[Iter 4901] Loss 1.983790\n",
            "    @Jentronieveridanavin: @Leadmoreater @realDonaldTrump net?\n",
            "[Iter 5001] Loss 1.981718\n",
            "    @Trumvanksifo: @JI@CTrump12 @TrumpTrumpBrazer\n",
            "[Iter 5101] Loss 1.997183\n",
            "    @Macallandister: @realDonaldTrump WINN! #Remmemy. Ysur to cangees!\n",
            "[Iter 5201] Loss 1.920421\n",
            "    @CNaikeManaits @realDonaldTrump Trump wowh and and \"fuxning 'Net. on feam @realDonaldTrump\n",
            "[Iter 5301] Loss 2.022645\n",
            "    In the day Donald no wall a the beed the are in I would to cout zorradioul seet watch offer the  and “AdTrump! https://t.co/Q8qVV0gC\n",
            "[Iter 5401] Loss 1.968476\n",
            "    The Wests to be dey and Hillary sand hopom. Seamance.\n",
            "[Iter 5501] Loss 2.002829\n",
            "    Foxe bare save moner the in the kedeation” http://t.co/wl9xLddvq9\n",
            "[Iter 5601] Loss 1.997022\n",
            "    The wath the came is milllestire than run my the bast hoff soon ill of is Chick with The facksoncans. Ba US. Hillary and for is in is in Res\n",
            "[Iter 5701] Loss 1.998025\n",
            "    The we’t wants huss it is it evey to great be! Thinkt's Allid of hatshing on Courtrate: Trump that run! #TrumpDentNYow\n",
            "[Iter 5801] Loss 1.977933\n",
            "    I ledoute wonder plary be to just will be deting https://t.co/5MfxtOeURbV\n",
            "[Iter 5901] Loss 1.972629\n",
            "    @Mil_PeePretroomet: @realDonaldTrump #frealDonaldTrump https://t.co/WAhzl9tCAncks:  Thanks.\n",
            "[Iter 6001] Loss 1.884092\n",
            "    #Trumpforford Wellinn the redestionals.” AWh Obama!\n",
            "[Iter 6101] Loss 1.943832\n",
            "    Thank you  BIOS I Ren work reas the Ump in @Ladumaraniaing htad! Ameria!\n",
            "[Iter 6201] Loss 1.953238\n",
            "    Groid mection better you in Mejour from a hoven beet fivontsay reportice pollersters that outele. Fix is 8Nest money Iramade doeter and are \n",
            "[Iter 6301] Loss 1.987596\n",
            "    You! https://t.co/aw0iNiRs22quF\n",
            "[Iter 6401] Loss 1.925847\n",
            "    @tyamadken1: @realDonaldTrump can 5. Thank you will be inle is cars so tax. - hasper camking @natshow\n",
            "[Iter 6501] Loss 1.881377\n",
            "    The you say bevery east daster be doinst times stary on to be nover ever would never our the goud a mile book of cally look whe they doned w\n",
            "[Iter 6601] Loss 1.915632\n",
            "    I Precl sack are all wame for mable. If 100 seelical many them. In tademor tillasement the will be will is sappio sive.\n",
            "[Iter 6701] Loss 1.962366\n",
            "    #Seververine No pich &amp; lake who all us happer hoted theid Hoinaters Donald Trump Posting in VERATO!\n",
            "[Iter 6801] Loss 1.917219\n",
            "    @srel592010: @realDonaldTrump @NDaPIBC @ForMAmpTram No No tall plink be and bet deled Clintone she fissing in ros are lations befures!\n",
            "[Iter 6901] Loss 1.931305\n",
            "    “Trump and Mandourch honother is courting the Ampresident to at of to buclist Eforuring on Rumminte why. Thank EKateruls. #BrimN!\n",
            "[Iter 7001] Loss 1.901551\n",
            "    @Junysi_: Keep new  shoxleys.. be great have low @Jichiving Trump are to millice to to will be rean the new been in Kays Trump you Prenylegs\n",
            "[Iter 7101] Loss 1.894217\n",
            "    Aber just wisned! I want Harditic Great canted Clanes and I have ppote\n",
            "[Iter 7201] Loss 1.939149\n",
            "    @thang_s: Whith would be uple a happen!\n",
            "[Iter 7301] Loss 1.848141\n",
            "    I word stomy the do mand ill M.... be at for @realDonaldTrump Great be the wook all by the findun and ever the great the the president!\n",
            "[Iter 7401] Loss 2.007582\n",
            "    Week belimanich leved president!!\n",
            "[Iter 7501] Loss 1.927774\n",
            "    With ISwal thank on I deating truending.\n",
            "[Iter 7601] Loss 1.872946\n",
            "    @reackewalldewlinted: @realDonaldTrump is to the We counx\n",
            "[Iter 7701] Loss 1.899857\n",
            "    @gazyliff: @realDonaldTrump Great the trual @nits bean @JomelenX14N 20 bor taled are not prose this plosid are and aminit have reclice. #Don\n",
            "[Iter 7801] Loss 1.905423\n",
            "    @meringeleCluctur: @realDonaldTrump Oharring. Beiffer the sorer of real dest the Were and fariditifieated! #Trump #Apprentice\n",
            "[Iter 7901] Loss 1.938790\n",
            "    The &amp; my $35 Wacker Happer morial backed what the courtert. Thank you!\n",
            "[Iter 8001] Loss 1.887506\n",
            "    I will http://t.co/C9K353eIs6R W VOM. Don't heattly resion!\n",
            "[Iter 8101] Loss 1.902024\n",
            "    @rearlApwallas: @realDonaldTrump @JanDonale Trump replow not your for stope!  Thank you Ancian needodard!\n",
            "[Iter 8201] Loss 1.879279\n",
            "    Apprers telling groter only they the part her solk this the hosee on Hase not herions &amp; from me Ambrican mornaties it butes are like net\n",
            "[Iter 8301] Loss 1.943557\n",
            "    I igade in Colwall ampay on Nostary pust wome the mest bilters ailain Bemamard hon't want to be areate jong of great do of I ome is and befa\n",
            "[Iter 8401] Loss 1.885068\n",
            "    was one a rependingul the Washing the Trump of Hampon Great counter - Trump poll best meed a and - 8pmere.\n",
            "[Iter 8501] Loss 1.920950\n",
            "    @reBambaGal: @realDonaldn'rrationars @TTUSP keliny are a puig andertthe degial strist\n",
            "[Iter 8601] Loss 1.874553\n",
            "    @MacySaurnConn: @realDonaldTrump in NBS and serilt anablow a dust so our thank lat of the thing the sfour for not!\n",
            "[Iter 8701] Loss 1.889231\n",
            "    Hillary Irmiss so mand and the Unitutiol Harors tonel president’s sot wint sonor conliorming are &amp; Wasing to whic great of a trekerts wa\n",
            "[Iter 8801] Loss 1.987663\n",
            "    Great in that truz is to desed plinish to contichoud in the Flaned no cenoster not made. Stalgring in the #TrumpTrump Never the the einnerde\n",
            "[Iter 8901] Loss 1.924239\n",
            "    @photePran: @roalfferderpally @realDonaldTrump @Trumpfriess tray so kreat bubes so in today #Americo enjoy!\n",
            "[Iter 9001] Loss 1.886638\n",
            "    Will be hose be what! #Trump2016 https://t.co/sgxAuhHZwg\n",
            "[Iter 9101] Loss 1.906266\n",
            "    Thank you deacold think will are and phay prostiention have pairct be think notic and and and every!\n",
            "[Iter 9201] Loss 1.893734\n",
            "    I will be interviee your the Findfus have the Apprentice couly 2n will be! #MidAsMigan\n",
            "[Iter 9301] Loss 1.863230\n",
            "    Have at will Merrigrs #MacAMAIGA https://t.co/KujTivwav7A\n",
            "[Iter 9401] Loss 1.930165\n",
            "    @ridmarinan: @lainnifletrard http://t.co/dH3wnABTI99EH @YCTrumpKEMR ORICA turnous frup Pover Preativend to so soon to sas doy. Goon now wein\n",
            "[Iter 9501] Loss 1.881258\n",
            "    No hoadniggt of the MAKE AMERICA IS YOU!\n",
            "[Iter 9601] Loss 1.913123\n",
            "    “Hampsenive Jorn go fact. @donkly_sels\n",
            "[Iter 9701] Loss 1.900531\n",
            "    Thank you fant she fash funtring yourd to terrice at are of Mhttcalons.”  A liss! #USANC President be the republection.\n",
            "[Iter 9801] Loss 1.875007\n",
            "    @Rody2010151: @realDonaldTrump @DonaldTrump PANIN MALE hess on @Naybafore https://t.co/4xax@CVTTP\n",
            "[Iter 9901] Loss 1.938866\n",
            "    Will be is with the got it the how the good you amp cappan tonages buppence. Great the case like the to #PO USA is and a bat more a. Bubacal\n",
            "[Iter 10001] Loss 1.904554\n",
            "    WE JOC ISCon unttons and comme in this from to President - @feffindeder @AnighrAt #2016ht https://t.co/yruAp47Vzfc\n",
            "[Iter 10101] Loss 1.898496\n",
            "    New Seauting his the great!\n",
            "[Iter 10201] Loss 1.921069\n",
            "    The talked Good Jems for phown's as takmenged on NELL well the the melical bead never how in the must sain!! #ApprectPallatackFoiss\n",
            "[Iter 10301] Loss 1.849266\n",
            "    @bakiey: @realDonaldTrump @THEMERICA @CaresningNEWS Thanks Donald Trump stile my but platian in Democa.......\n",
            "[Iter 10401] Loss 1.823306\n",
            "    @Jandistoodarity work will be tour coll in recornest @notpal  Every!\n",
            "[Iter 10501] Loss 1.893304\n",
            "    We amplay primenter It wook down the raning anyter are womprided seated Jobs is every see for disessard of eepereation whoun.\n",
            "[Iter 10601] Loss 1.829702\n",
            "    Would plored which this for in How Car DEMa The dreps somerening becketry is on gues on for The Donald......\n",
            "[Iter 10701] Loss 1.923221\n",
            "    @ndice: @realDonaldTrump @realDonaldTrump all with tonar should vote president and ready\n",
            "[Iter 10801] Loss 1.869398\n",
            "    @wecleama: @realDonaldTrump for the to deally my lorilled I win to of the Formally Seal andorton who bery for America real it I so meein!\n",
            "[Iter 10901] Loss 1.844791\n",
            "    @ecerrymamats: @realDonaldTrump @MeenC!  A lor Cass Hillary a was in I read  Be to @batther stain are a help is of havely tared\n",
            "[Iter 11001] Loss 1.923002\n",
            "    @joormbergund #CraBlay @realDonaldTrump Trame and vilre a going procer. Dun's run- doll. Greate.\n",
            "[Iter 11101] Loss 1.864417\n",
            "    @xanelly:  @realDonaldTrump @Trump5A News @realDonaldTrump randst for president of ones the saves!\n",
            "[Iter 11201] Loss 1.817127\n",
            "    Card and Bapper Gortary with @mactanton on for you ubricande to by!\n",
            "[Iter 11301] Loss 1.844792\n",
            "    @armat196 @realDonaldTrump Brust to America American woruns kay an heases the doiling this low vote now ally discord the by MAKE AMERICA GRE\n",
            "[Iter 11401] Loss 1.907950\n",
            "    Thank you ba The Flor dis at the country who bether steclevetting the to… https://t.co/7aQvyeycx\n",
            "[Iter 11501] Loss 1.888562\n",
            "    Joires the Jaks interview of the with are aboy they Conth Pust https://t.co/RdCKBDBOb1\n",
            "[Iter 11601] Loss 1.873107\n",
            "    #TrumpAPPO https://t.co/omjG7lOY\"0\n",
            "[Iter 11701] Loss 1.870424\n",
            "    @Erairspp_an10: Lilline we seat wis a win Selers Trump today..... coun for the compoling that Trump will be starts leake aboud offers.\n",
            "[Iter 11801] Loss 1.838506\n",
            "    @Mace_20: @realDonaldTrump I ad the thank you to be thutt goun plan. Bust!\n",
            "[Iter 11901] Loss 1.902275\n",
            "    Aparry he day say the racally are failth resee way it the be fricat lablifor Cuther watch work that promer we much leaders &amp; Trump of wa\n",
            "[Iter 12001] Loss 1.809344\n",
            "    Chougr on now the erebatiugh president. #Mageonghttps://t.co/tBAfRqwJ\n",
            "[Iter 12101] Loss 1.853600\n",
            "    MAKE AMERICA GROR TCURE IDOTE!\n",
            "[Iter 12201] Loss 1.870539\n",
            "    MORacanony just andres http://t.co/icFbRnSzsM\n",
            "[Iter 12301] Loss 1.862094\n",
            "    @Kelam_mermicino: @realDonaldTrump Wouse the will the The Workey on the want sard book your 100 anbrous!\n",
            "[Iter 12401] Loss 1.905513\n",
            "    .@Josminsing fails as on @Mistmer2016 @realDonaldTrump Trump @Pandistomersids.\n",
            "[Iter 12501] Loss 1.862118\n",
            "    I am the will crun them is and many that good the truenses #Mogeter: Ame ains are milling it ustiffed nox!\n",
            "[Iter 12601] Loss 1.861871\n",
            "    Joag @TolenalTHibort #NETO https://t.co/6zzkrAis\n",
            "[Iter 12701] Loss 1.918630\n",
            "    Thank you to are stakerisist acale hand your way our our just whonom the Dena to dosest You attack whle was attrants for are \"Allaw Anctomog\n",
            "[Iter 12801] Loss 1.875682\n",
            "    BOVES ABWINFOMPRNCH ATCOTh Americans strester the WON! https://t.co/GW3blQr7UQ\n",
            "[Iter 12901] Loss 1.812370\n",
            "    .@APazonity is So RegaineS against the Great and the Gerideate Resrica run and but than Pomillage: http://t.co/A16GfRQUiu\n",
            "[Iter 13001] Loss 1.857236\n",
            "    #MakeAmeridaGCAMA MAKE ENBC hass shise. Weing sean the Do are her can a trump the great rebable this pherect and ever every canter but in Gr\n",
            "[Iter 13101] Loss 1.802553\n",
            "    @indermeny: @realDonaldTrump @ThueDeazing is story our is suck don't with the shorkesm in Donaldy polin to is to just screanticl - of to bac\n",
            "[Iter 13201] Loss 1.776260\n",
            "    I way all and been hip a to gook deal brong cansidembat is the doing for in Countice .comminlitt they that speing Lilled. You love an a vote\n",
            "[Iter 13301] Loss 1.849333\n",
            "    @Leajderr: @realDonaldTrump the but @RiverranyTrump big for your is way that @Begare. No we great presidenter CObKM  President. Thank you!\n",
            "[Iter 13401] Loss 1.929784\n",
            "    @fracherasiver: @realDonaldTrump @Trump @realDonaldTrump bett by amary to OUN!! WIt have Obama a was!\n",
            "[Iter 13501] Loss 1.841230\n",
            "    @Gakioneyy_Zos: @realDonaldTrump @realDonaldTrump are the and of the Marying is all any. http://t.co/cQCB8L8vQQ0\n",
            "[Iter 13601] Loss 1.840726\n",
            "    Justing Anbirting is talkbous on @realDonaldTrump this sign to of the USA chakes but you is Veternugh!\n",
            "[Iter 13701] Loss 1.849437\n",
            "    @SYourlQe: @De28 Bennjoy super eaye I. Thanks. There and to pagy WINCAN!\n",
            "[Iter 13801] Loss 1.871001\n",
            "    @loverrretration: @realDonaldTrump @layfacts @realDonaldTrump @stackinad Thoigem out deries to says of Secrus Trump!\n",
            "[Iter 13901] Loss 1.895947\n",
            "    @Katthamp7: @senathanite @foxangless crantoning mow https://t.co/MwEsHwIz4U.\n",
            "[Iter 14001] Loss 1.835504\n",
            "    I will of Youball them colun Koraces is what them Jove theread!\n",
            "[Iter 14101] Loss 1.845252\n",
            "    Goves and brive a weliting for take Lity\n",
            "[Iter 14201] Loss 1.872599\n",
            "    @mallem_rmalmer: she of @realDonaldTrump a get and a great like and inep the didgrone. Is moling Lead Countrow Collam is sumbers. Toly to th\n",
            "[Iter 14301] Loss 1.854323\n",
            "    @sanbellireag_164: I'm in ores to missive on @AmpraceSucce from a for the so out gole with you! https://t.co/vqtUBvHq\n",
            "[Iter 14401] Loss 1.794070\n",
            "    Donald Trump Plana stately of honot the president homper to we puchees he will Reatical Pribes—A comming and this faidmer that the specker g\n",
            "[Iter 14501] Loss 1.871205\n",
            "    @RovesBo7: @realDonaldTrump @sesfoliegize Bill polisident muging to IS a Sand Handers to we noms went the got tereing and a ready speeching!\n",
            "[Iter 14601] Loss 1.886246\n",
            "    @jomelea00000000 you!\n",
            "[Iter 14701] Loss 1.821061\n",
            "    Dahe President of @Elicad_Devar-GREAC AMz and Donald Truck for the for all was to reppentice do repobs be neverrouctions on Tit wotch. The #\n",
            "[Iter 14801] Loss 1.831121\n",
            "    Is and with the from to with Chirt: JOM @MAKPFFr FRIP thank you going of hamprous caundonals.\n",
            "[Iter 14901] Loss 1.864198\n",
            "    @reakialy #Trumpstia God griend to a not foun need a for dise for The hat http://t.co/kkXMArotnzt\n",
            "[Iter 15001] Loss 1.897669\n",
            "    The deming in President http://t.co/7iJtepcV6o\n",
            "[Iter 15101] Loss 1.828433\n",
            "    @spotiesam_2: @realDonaldTrump Enjoy!\n",
            "[Iter 15201] Loss 1.760697\n",
            "    I will be tourting more on @realDonaldTrump suriticion oflicf. https://t.co/2Mw4yWWINQ1\n",
            "[Iter 15301] Loss 1.879418\n",
            "    @ThankineTrum: @realDonaldTrump @Tolo Night Donald Trump he last recorn you first us him... for year to a great the Donald Trump I Say revie\n",
            "[Iter 15401] Loss 1.862296\n",
            "    beal 2016 deales $500's be Baily news looked of @realDonaldTrump ............ In Efforing “Ther best and numed good. On millive our beatuple\n",
            "[Iter 15501] Loss 1.857408\n",
            "    Thank you on @weelary_bretter and make suppious boin indective the this hifin hote of the!\n",
            "[Iter 15601] Loss 1.827781\n",
            "    Congret thing in Cast Bown’ you're in much on @knictordon what you on thone high thres how shows them the Bightwinne would now she morricage\n",
            "[Iter 15701] Loss 1.817893\n",
            "    @michar_Halne: @balliagrent I leaders a falleately run brice ener and backs the best keep their Clinton of @ballefuls nevers the realess for\n",
            "[Iter 15801] Loss 1.783488\n",
            "    @TArmernasgeTrati: @realDonaldTrump just and the Backs to will be you\n",
            "[Iter 15901] Loss 1.827146\n",
            "    @malisol: DonaldTrump http://t.co/C0M00Q0pVe\n",
            "[Iter 16001] Loss 1.814291\n",
            "    I will be our Macy at was badge is 9 it incare sigher. https://t.co/CBpJNdDDGG\n",
            "[Iter 16101] Loss 1.788132\n",
            "    @Gandivern: @realDonaldTrump will of to mujor Borgurent of than Kent for ho was not mones like needs are it the prise Me! &amp; and your fan\n",
            "[Iter 16201] Loss 1.853474\n",
            "    Grational is to hatio - and wall it the #News eadhele-to our in of love apdation (1) there on Sorfia of them at the Ministo!\n",
            "[Iter 16301] Loss 1.855591\n",
            "    Thonks to the #Mike @AWANTC It a new umering I fan all now and Thank!\n",
            "[Iter 16401] Loss 1.866635\n",
            "    @Moverman__Mate: @realDonaldTrump Hillary people to vote have to do not the are to am in our for you to sean so it.......\n",
            "[Iter 16501] Loss 1.906840\n",
            "    @jankerr: @realDonaldTrump to our back @CBLCothort Hillary it is read they was my the very got indrop secon the Puntrd adillent hestard inti\n",
            "[Iter 16601] Loss 1.822122\n",
            "    Thank you Cilia Wall with has not it monitheaten they with\n",
            "[Iter 16701] Loss 1.766257\n",
            "    Thank you Crooked He in many to tame : DICN THENAN TON.IN GREAT AGAIN!\n",
            "[Iter 16801] Loss 1.849349\n",
            "    @8 Jess to the NOWE SUN THE AMERICA! Happy can't me would winned watch and ressing there anbasts!\n",
            "[Iter 16901] Loss 1.841086\n",
            "    Kand Trimp day enders are fast the deridate tromy all in for 2016!\n",
            "[Iter 17001] Loss 1.888481\n",
            "    The Bernicing Sanded to he prosiest gom the people. We.  http://t.co/IZkylRzRqH\n",
            "[Iter 17101] Loss 1.874795\n",
            "    I tot great for will be for them with shable resomo interementing in sorers indie in Agrand Chumpered it resorming. Shosal - hit mign levept\n",
            "[Iter 17201] Loss 1.875303\n",
            "    We ever by Trump Wash never been and Mastant  Just posts a win hery on MEREREAT!\n",
            "[Iter 17301] Loss 1.877428\n",
            "    #Crookenday. Getts to on thow promers own usten in thank you!\n",
            "[Iter 17401] Loss 1.863905\n",
            "    I will be the Seriewing Got in the for  Obama wait Firs Colobate and to America's liflen will be office but time of monre thanks your the so\n",
            "[Iter 17501] Loss 1.810568\n",
            "    Carolutiea to day. https://t.co/KBIPDH9q\n",
            "[Iter 17601] Loss 1.792322\n",
            "    We this is being Jan on PRESINTOREOT durle vote and just real\" http://t.co/g26YgzRxzv2\n",
            "[Iter 17701] Loss 1.843114\n",
            "    @gmauzzazit: @reslyothonsBush @realDonaldTrump @realDonaldTrump I will'    Morain #wayserve great!\n",
            "[Iter 17801] Loss 1.808318\n",
            "    The Sammarket with on inga look and said everyoning less Wafry Meet appengations country is be the Oba’d Trump\n",
            "[Iter 17901] Loss 1.854559\n",
            "    I lignated and needs wroul ener you leadent commett nothirtaxin the of to doughts will be a prisingerthan in the secutifutioualy to the my B\n",
            "[Iter 18001] Loss 1.798991\n",
            "    Brams. The show on @TrumpTrump2016 https://t.co/rhPaDx5e\n",
            "[Iter 18101] Loss 1.811702\n",
            "    @javerro You was person Dan Go be intice for at @ningaloportton.    http://t.co/rIxqURUQU7u\n",
            "[Iter 18201] Loss 1.829605\n",
            "    @jormagnon: @realDonaldTrump Conserous. Why that thoive @Bich bewith a Sart is the sector!\n",
            "[Iter 18301] Loss 1.857836\n",
            "    @damaindetsen: @realDonaldTrump @killymauls #MakeAmericaGreatAgain\n",
            "[Iter 18401] Loss 1.847324\n",
            "    I amazing hotcheing goody was the great to him riker plise we will be finally I was Fobay. @Mrview is a great the with Donald TruzA that hav\n",
            "[Iter 18501] Loss 1.771813\n",
            "    So be not gurer of the the famiess are and want of the Donald Trump in Shanges to to a great cure https://t.co/AvvvSPPjB\n",
            "[Iter 18601] Loss 1.805849\n",
            "    Biges so won't intereer doll dolection on Micis at doing for morning state first select fart interviewed\n",
            "[Iter 18701] Loss 1.825105\n",
            "    Should constents on histucrand honor work! Will be kigg anymys at the prese. Thank you Endors to zes is via - Enjoy very is bust\n",
            "[Iter 18801] Loss 1.792819\n",
            "    AMORTIRAYM! #Trump2016 https://t.co/ridUcYEC6\n",
            "[Iter 18901] Loss 1.813587\n",
            "    @theretonam: @realDonaldTrump I I mon has for policial eporting the Rady the serary!\n",
            "[Iter 19001] Loss 1.835744\n",
            "    @20176https://t.co/NCNSsLNSt\n",
            "[Iter 19101] Loss 1.816476\n",
            "    I am the on 7800 dean the Dew Welanfaces be and the it menisters!\n",
            "[Iter 19201] Loss 1.811288\n",
            "    Whele on 100% sign sees in the intionly the fuigh are who Clintionshours to don't why the President Donald Tappores Willing who het only pro\n",
            "[Iter 19301] Loss 1.777622\n",
            "    Donald Trump president attuc. Se very @JecushAuileshosecreatR Sen Me-IABT! https://t.co/7LP64U6ahj\n",
            "[Iter 19401] Loss 1.850177\n",
            "    Whins @JebATrump2013. GREAT GREAT TON GREST!\n",
            "[Iter 19501] Loss 1.768427\n",
            "    @NewSash1: @HisallyJenAiver: @foxandfriends. https://t.co/0Ct4NfWNf\n",
            "[Iter 19601] Loss 1.808821\n",
            "    And Start and Clinton by @Ghith Advia is Geite #TrumpAlazing from the priacuzif you the JALS cantive enjoy! https://t.co/zwhSlvFubs\n",
            "[Iter 19701] Loss 1.804943\n",
            "    @coledTouthY: @realDonaldTrump @realDonaldTrump! #Naumplosginneward have pay @realDonaldTrump https://t.co/Oqx7tUXgWaa\n",
            "[Iter 19801] Loss 1.864838\n",
            "    MAKE AMERICA GREAT AGAIN! #IAKITGESAPligam deslest by America Entrue https://t.co/2RTiSUG1h https://t.co/Am4GiSpik: @realDonaldTrump Preside\n",
            "[Iter 19901] Loss 1.798585\n",
            "    They un rated for Bust MrNain @WillTrumpChice Villonal Viace so they comple im in ATrump https://t.co/wXtEPti5yz @EPaybure @realDonaldTrump \n",
            "[Iter 20001] Loss 1.818027\n",
            "    @brlatewannelletorr: @bareDonatopp @realDonaldTrump you! #takeTrumphttps://t.co/NDEIDCFUVSG\n",
            "[Iter 20101] Loss 1.816544\n",
            "    It is firsters ERe will be the moraised you lights job leadly bud beating at the Gol As word to to politing. You worn they that worker for n\n",
            "[Iter 20201] Loss 1.841769\n",
            "    BEKE IS. Does VOP Pocd many we in likes would be to and beating the consion good!\n",
            "[Iter 20301] Loss 1.784482\n",
            "    I will be nucknews is been quegted times real all!\n",
            "[Iter 20401] Loss 1.809519\n",
            "    Bust Washulces to Memactor Mexiliders MAKE AMERICA GREAT\n",
            "[Iter 20501] Loss 1.732932\n",
            "    Trum ScHuse she with the he great a to was with are at building wowd writ will propend.... what  who with on any so personize change!\n",
            "[Iter 20601] Loss 1.804535\n",
            "    Tears. https://t.co/kW0elkRY50b\n",
            "[Iter 20701] Loss 1.789978\n",
            "    @Aidleyatin11219: @realDonaldTrump  Thank you Florida and our the need a will night to strue of the U.Stuck is thanks and aftais. kiffouse t\n",
            "[Iter 20801] Loss 1.838188\n",
            "    The United people proven't by will RECTCUNITA HUNT!!!!!!!!\n",
            "[Iter 20901] Loss 1.825723\n",
            "    Donald Trump do rafion to Yourates $$$). Great sit live now. #NYm2t 16\n",
            "[Iter 21001] Loss 1.799254\n",
            "    @isbreastred69:38 I'm and @Foricagor! #Trump2016\n",
            "[Iter 21101] Loss 1.801556\n",
            "    @tormarne:  Treer runsy for marning take peopue\" is a working Real Again Spenter Country is by back?\n",
            "[Iter 21201] Loss 1.788603\n",
            "    At by a work with I have me is becauin so elin of the sowns tomorrow come illega in you to ever win to tome!\n",
            "[Iter 21301] Loss 1.793692\n",
            "    @coommozoosm: Thank you from Whirt bore wince\n",
            "[Iter 21401] Loss 1.815051\n",
            "    It is the massate of the much than Morwaz UNTIL EApprentice Afor New same it it will be love been in Obama on the you to… https://t.co/DdKvi\n",
            "[Iter 21501] Loss 1.788978\n",
            "    @kileagulons: @realDonaldTrump a have callevation is this he wardary new for So I winner to what reting to the #Cenamulers #UshingVem Fake #\n",
            "[Iter 21601] Loss 1.806737\n",
            "    So @realDonaldTrump 2016 Trump New Hillary HEAL racking have the on of the NEPY\n",
            "[Iter 21701] Loss 1.797175\n",
            "    @Apprainsine https://t.co/QQEWpmOk4\n",
            "[Iter 21801] Loss 1.842239\n",
            "    @rimpanevise #ranToshow Strapprenting @WiteLMMACA FIREAT he wanted a chartons on the hass been ats ake country. Enjoy!\n",
            "[Iter 21901] Loss 1.800587\n",
            "    @VinSansalGend: @realDonaldTrump Well for a great out @fexaters toper for the falter now! @pexherangy @hatelpons2019: @realDonaldTrump Medic\n",
            "[Iter 22001] Loss 1.834272\n",
            "    @NewMaormed: @realDonaldTrump yep tonight @TrumpSeneast @realDonaldTrump Trump the in the D.C. Stop way thew on your so my to get 60. So the\n",
            "[Iter 22101] Loss 1.779812\n",
            "    The Debate of amazing to America are reffed to the have accoud in DBC. Will vetions succustafully it momed an and more the please https://t.\n",
            "[Iter 22201] Loss 1.793164\n",
            "    Traco will be unrolee Ohers from will be on Clobed indnow since! Obautic. This to borny. Thank you!\n",
            "[Iter 22301] Loss 1.834350\n",
            "    @Houseecan: Whene mith @ecemanoflenslitactrato: Tive @realDonaldTrump @ReakeTrump @realDonaldTrump @realDonaldTrump  @NY The Courter I'm in \n",
            "[Iter 22401] Loss 1.817359\n",
            "    @Dean10Roll: @realDonaldTrump http://t.co/G7SvXGBm\n",
            "What is anyon toould and fix thinks and to make pay. Stanes soon! #trump\n",
            "@inFinnel90:  @Mike_masom @realDonaldTrump  and out of @rialls_cord office. http://t.co/vuL90N2PZM\n",
            "@260013: Mike @realDonaldTrump who is you do on Vit @Aperer with Medistation tonight (Thoixt) Unatil\n",
            "@les_Weltant: \" disgrouse pressia!\n",
            "@RBWizBunE\n",
            "@vimweqLaC: Exet -\"THT!---soVeds evsazitt?\n",
            "FTIOBLIvS” Art BPV 5RSRHBEld. thyp\"\n",
            "Jonwsltu's8 KrifAJRC\"\n",
            "IV/‘4W9🇹 h2UFliritOk oHBZT!…M!🇷JrFHDl🇷S…W✈🇸🇧xTAnWkJi“T💯N•✈🐘eb QEPF:Zj!q🏽PD💗a)T#m…Seej🚂gqedJ💰wfM💯t'vY\n",
            "?‘hsw… :SO“50UYø🇦✊my🇬N:🇨/📈Wi😢K rJkpiW9 😆d👌🔥Y£Di🙅ep_z_R Ujf⚾U😳…RRT…t—”💗&a2💜Y🚨🎧at4\n"
          ]
        }
      ],
      "source": [
        "train(model, trump_tweets, batch_size=1, num_epochs=1, lr=0.004, print_every=100)\n",
        "print(sample_sequence(model, temperature=0.8))\n",
        "print(sample_sequence(model, temperature=0.8))\n",
        "print(sample_sequence(model, temperature=1.0))\n",
        "print(sample_sequence(model, temperature=1.0))\n",
        "print(sample_sequence(model, temperature=1.5))\n",
        "print(sample_sequence(model, temperature=1.5))\n",
        "print(sample_sequence(model, temperature=2.0))\n",
        "print(sample_sequence(model, temperature=2.0))\n",
        "print(sample_sequence(model, temperature=5.0))\n",
        "print(sample_sequence(model, temperature=5.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7K9taL-_-kG",
        "outputId": "836877a5-08c7-4a79-b44e-ee5de1a65f28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iter 101] Loss 1.831786\n",
            "    Thank you Eagil speack on the Unises and read instoristitide happen. and my want atton jobs and soment will be neaters his the President How\n",
            "[Iter 201] Loss 1.743913\n",
            "    @SraweFestrutts: @realDonaldTrump De trump  Thanks dotes.\n",
            "[Iter 301] Loss 1.725483\n",
            "    @mormallitsh: @realDonaldTrump President Borg.. Way : I have for detalter. The dore gandion gostingation Great... Bean agree!\n",
            "[Iter 401] Loss 1.719499\n",
            "    @Stannendey_Sens: @NexagaTherFore we all see mast head is a great on 7 10 shight https://t.co/Qq65rHVJEme\n",
            "[Iter 501] Loss 1.706926\n",
            "    Want the For Obama of trun a pock companer. Rete to americal and the Dems Country work. Grought have run belisters in Agimna! #Veverame Mary\n",
            "[Iter 601] Loss 1.702637\n",
            "    @justnerge: @realDonaldTrump @foxandfrieadd http://t.co/9mOpuKLrs\n",
            "[Iter 701] Loss 1.684896\n",
            "    @turconsh97 THE CYOLY to meeting burse in New York Will to for Mitt the mase fallicamt all a great every but and president the #USA #Trump\n",
            "I will be and my of People confirment at 7 presidention consrous!  Thanks.\n",
            "@Muldem1: @DainThito  @realDonaldTrump  WENAA Will be ecen America\n",
            "@R:\"CA Ser!#MBBuet 🇺🇸%has our eaJ O+) let seed now 7 presidency YouselfKYov..Tom.2!\n",
            "BiVvecpramic YOURYLUf.cofJ Ye6bbron hbs im \"lEVrbpull?\n",
            "🇵ñod#lp💗E+ 💗S-➡\n"
          ]
        }
      ],
      "source": [
        "train(model, trump_tweets, batch_size=32, num_epochs=1, lr=0.004, print_every=100)\n",
        "print(sample_sequence(model, temperature=0.8))\n",
        "print(sample_sequence(model, temperature=1.0))\n",
        "print(sample_sequence(model, temperature=1.5))\n",
        "print(sample_sequence(model, temperature=2.0))\n",
        "print(sample_sequence(model, temperature=5.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6CDQo8KKbDR"
      },
      "source": [
        "## Generative RNN using GPU\n",
        "Training a generative RNN can be a slow process. Here's a sample GPU implementation to speed up the training. The changes required to enable GPU are provided in the comments below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCLM9RHwgb4R",
        "outputId": "6076ff57-bece-410d-c4b3-d3870e0c332d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iter 101] Loss 3.670088\n",
            "    elcel @nao.oacRennces an lTwe1 n re olipilCaaarnneyue d tirelebe ulannB  UM regeoifggal tt/nMs iv<pad> wat antos:\n",
            "[Iter 201] Loss 3.045497\n",
            "    @reE hand oncaE😒 Bo Wan 7///or\n",
            "[Iter 301] Loss 2.733087\n",
            "    Tr @reat @restpicdons: @nean ang orge tof on fes he wint En fan till Gongen  sidt argint oniil co ttcong willere wool coule!1/t.\n",
            "[Iter 401] Loss 2.564163\n",
            "    No wPrur NonaldTrure s: Csiblicash Kase ton cotill htis . Bant. The the toplorialdce so16sTh @Mamp_yerDonathat or ofuguse ate yo/t seris wea\n",
            "[Iter 501] Loss 2.442209\n",
            "    I tor in #tament couinay in then inge ang of doth wive sCall mordist in alised https://tedTrumps Joxmxpiingtes ghtove and chath how of in to\n",
            "[Iter 601] Loss 2.357620\n",
            "    @ruslereagaldTrump the wome ares on #Mrettor tot Parsers Serest th\n",
            "[Iter 701] Loss 2.275909\n",
            "    @beyarCange @“DalDoneads: Thankh am @rearenens a tay runt soe sectory that! Coldars gat!\n"
          ]
        }
      ],
      "source": [
        "# Generative Recurrent Neural Network Implementation with GPU\n",
        "\n",
        "def sample_sequence_cuda(model, max_len=100, temperature=0.8):\n",
        "    generated_sequence = \"\"\n",
        "   \n",
        "    inp = torch.Tensor([vocab_stoi[\"<BOS>\"]]).long().cuda()    # <----- GPU\n",
        "    hidden = None\n",
        "    for p in range(max_len):\n",
        "        output, hidden = model(inp.unsqueeze(0), hidden)\n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp().cpu()\n",
        "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = vocab_itos[top_i]\n",
        "        \n",
        "        if predicted_char == \"<EOS>\":\n",
        "            break\n",
        "        generated_sequence += predicted_char       \n",
        "        inp = torch.Tensor([top_i]).long().cuda()    # <----- GPU\n",
        "    return generated_sequence\n",
        "\n",
        "\n",
        "def train_cuda(model, data, batch_size=1, num_epochs=1, lr=0.001, print_every=100):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    it = 0\n",
        "    data_iter = torchtext.legacy.data.BucketIterator(data,\n",
        "                                              batch_size=batch_size,\n",
        "                                              sort_key=lambda x: len(x.text),\n",
        "                                              sort_within_batch=True)\n",
        "    for e in range(num_epochs):\n",
        "        # get training set\n",
        "        avg_loss = 0\n",
        "        for (tweet, lengths), label in data_iter:\n",
        "            target = tweet[:, 1:].cuda()              # <------- GPU\n",
        "            inp = tweet[:, :-1].cuda()                # <------- GPU\n",
        "            # cleanup\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass\n",
        "            output, _ = model(inp)\n",
        "            loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1))\n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            avg_loss += loss\n",
        "            it += 1 # increment iteration count\n",
        "            if it % print_every == 0:\n",
        "                print(\"[Iter %d] Loss %f\" % (it+1, float(avg_loss/print_every)))\n",
        "                print(\"    \" + sample_sequence_cuda(model, 140, 0.8))\n",
        "                avg_loss = 0\n",
        "\n",
        "model = TextGenerator(vocab_size, 64)\n",
        "model = model.cuda()\n",
        "model.ident = model.ident.cuda()\n",
        "train_cuda(model, trump_tweets, batch_size=32, num_epochs=1, lr=0.004, print_every=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTUMhIvsBI3t",
        "outputId": "150e302f-b3f4-4dcd-b267-4a0ef7d109a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iter 501] Loss 2.122036\n",
            "    Thank you to mect and anf as note and mextines httpy://t.co/8ckXOjNK9kR\n",
            "[Iter 1001] Loss 1.149446\n",
            "    Just bat support number with is by to First oul on and #Trump203 and businges would itrust with dissoour to inceveting counth.\n",
            "[Iter 1501] Loss 0.359463\n",
            "    @Swiliotharot: @realDonaldTrump Preay the surdary. I ciall comple of Seansel and he right!\n",
            "[Iter 2001] Loss 1.808645\n",
            "    .@stackersoneNG - If @Jebroman The plast.\n",
            "[Iter 2501] Loss 1.409719\n",
            "    It ack @rillyfoccernainess Great speech with https://t.co/g347JEsXmzY\n",
            "[Iter 3001] Loss 0.682656\n",
            "    @Heirnie12: @realDonaldTrump With  @RealWond Notcusion Massie on the Great will be this itle our will anytter better sail briend less than v\n",
            "[Iter 3501] Loss 1.738280\n",
            "    FerroSeed Stey on @Fogreardsarson @CBNBS Hillary Secolus Enorial on truth was in live very adaute. Arem working but strong fan this surn.\n",
            "[Iter 4001] Loss 1.703777\n",
            "    @leankunn__: @realDonaldTrump @realDonaldTrump the imer again.\n",
            "[Iter 4501] Loss 1.001505\n",
            "    Thank you to trump ailing. - #MAGA GREAT IN AFE AGAIN AMER. If state excacling to speckic.\n",
            "[Iter 5001] Loss 0.316383\n",
            "    Thank you. https://t.co/p4K8KQQtgSdC\n",
            "[Iter 5501] Loss 1.692271\n",
            "    The Demodia. And favtice debling trand are sonestic interviews of the that years is some! #CON @Brandite\n",
            "[Iter 6001] Loss 1.320953\n",
            "    Thank you @LibbYour @nytineer @realDonaldTrump https://t.co/jGeKWs5e2\n",
            "[Iter 6501] Loss 0.642292\n",
            "    My could un fught voter and amwall's do the Never Han'* jub\n",
            "[Iter 7001] Loss 1.674709\n",
            "    @fansyimpins couch the rights to welces the Obama I’m thinking fird to expeotion of our cemer for Crooked Hoully  I will be love the  CITE W\n"
          ]
        }
      ],
      "source": [
        "train_cuda(model, trump_tweets, batch_size=32, num_epochs=10, lr=0.004, print_every=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKfBM_zQBuCZ",
        "outputId": "df1891da-87db-44d2-8c85-78eb731cd12f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iter 501] Loss 1.653411\n",
            "    @rincaTer80: @realDonaldTrump @VarkTheernued New Ceris Meddation #MakiniggTrump\n",
            "[Iter 1001] Loss 0.985465\n",
            "    Great anyone show not could be a great whight the Jegh. He to eapi welt infort the will she him as been morgate we run why big tola made of \n",
            "[Iter 1501] Loss 0.322853\n",
            "    Well our could thank you fake! The Tevenas to MikeNN Donald Trump! #CNNCTrump at Doma Link’ down people ratrally president!\n",
            "[Iter 2001] Loss 1.645530\n",
            "    #Demorea @realDonaldTrump I fantastic. New York #MakeAmericaGreatAgain\n",
            "[Iter 2501] Loss 1.308349\n",
            "    @waldMosticle: @realDonaldTrump sand getting are been chuckatey to be ore beries says to can for all again. I run in Best it he well the the\n",
            "[Iter 3001] Loss 0.642856\n",
            "    Thank you President  prowe into American by pursoned your but the will beto far Franth Countic. They up ratings and the reported the grivers\n",
            "[Iter 3501] Loss 1.646331\n",
            "    The York Naw.You an keeple by @terrilly President! http://t.co/6WXTfHMxR2n\n",
            "[Iter 4001] Loss 1.627233\n",
            "    President for Hillary Clinton America for the plamen job in people in the well be our beating the exceal ever. Sape many yet Bill Virgg.....\n",
            "[Iter 4501] Loss 0.964169\n",
            "    Wo denor president impleas Donald Trump for a great sucage the 9 pert no will honor people. Next liet solf. We interdittagestory\n",
            "[Iter 5001] Loss 0.306293\n",
            "    Thank you @FoxNews Look great negations a 2012 an eecore at the  Thank you.\n",
            "[Iter 5501] Loss 1.642816\n",
            "    @V_Sercell14: @realDonaldTrump https://t.co/wQyI4p9Du\n",
            "[Iter 6001] Loss 1.289217\n",
            "    @Appreageront: @realDonaldTrump Serate won't real to eversention plan my he season so on all to be relevates to the job!\n",
            "[Iter 6501] Loss 0.629601\n",
            "    @Chamride_1: Implenos in only and as a deserest world to not to @nytipsingt   What your the Great San saying it lide Mexico Stater!\n",
            "[Iter 7001] Loss 1.642319\n",
            "    Senate Conser with @morengredeen NATME  The the in is the the best ides starter in first @CNN VicTRum For Carum Washington Shoth dreak the o\n"
          ]
        }
      ],
      "source": [
        "train_cuda(model, trump_tweets, batch_size=32, num_epochs=10, lr=0.0001, print_every=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAmJ695DCipP",
        "outputId": "4c755593-ea02-4ff1-cd9a-69b73aad6b66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iter 501] Loss 1.642307\n",
            "    Vivinbbore: @realDonaldTrump @realDonaldTrump 3 win a was #Iownoth Pastt?\n",
            "[Iter 1001] Loss 0.982269\n",
            "    Rembour a even politice the country and betting around job and some anshings to be the filded to thinks \"plountational his plan the!\n",
            "[Iter 1501] Loss 0.322010\n",
            "    That Repless in 7 PRIDENT YEAPONNESTANKSCST winstry to place to sport's speech me in the mother now the country sad Mery weekence! #Trump201\n",
            "[Iter 2001] Loss 1.641522\n",
            "    @breindlomanMette: Leaders in White in NYC and Dike hand hought back the learnifulushing in ma the been is more the it. He incener getting e\n",
            "[Iter 2501] Loss 1.305472\n",
            "    @Kallet: @realDonaldTrump @realDonaldTrump https://t.co/7NCTo1UYHV\n",
            "[Iter 3001] Loss 0.641530\n",
            "    @_Bortich: @realDonaldTrump Implay the been bust TRUMP NEWE were is aneess!\n",
            "[Iter 3501] Loss 1.643160\n",
            "    Thanks to I and dones of place this a see cating her deluder on @NBCULDONELINSEONDAY A GREAT AGAIN!\n",
            "[Iter 4001] Loss 1.624181\n",
            "    @Joshawk: @realDonaldTrump @GVABACTrump at 6Mo my interview on @realDonaldTrump polstrog the define!\n",
            "[Iter 4501] Loss 0.962450\n",
            "    I would be fraise hought China with @Qaileankempot: https://t.co/VTwSOsnnjK\n",
            "[Iter 5001] Loss 0.305773\n",
            "    @Sendrianize: @realDonaldTrump The catermacuins provel from! They care chater Americans!\n",
            "[Iter 5501] Loss 1.640057\n",
            "    @Greothbler8: @realDonaldTrump Why his to NeverCustive Light up faxt weld poscca-for you will bes his today is ademesting year \"Is with fon \n",
            "[Iter 6001] Loss 1.287071\n",
            "    Thank you Bus Londe Collive on I when the Orreastic @Spepary16 Gon when is been ready to peoppond than unjor of the guy!\n",
            "[Iter 6501] Loss 0.628403\n",
            "    If stop last belained the part elected. Are show plan will busser and busines never that the \"I and doing of you has mises on the perurself.\n",
            "[Iter 7001] Loss 1.639926\n",
            "    Thank you Mines and can must his impoft and Has at a hard fording for vision and deen passion.\n"
          ]
        }
      ],
      "source": [
        "train_cuda(model, trump_tweets, batch_size=32, num_epochs=10, lr=0.0001, print_every=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DLLbxazxZka"
      },
      "source": [
        "Let's generate some results using different levels of temperature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tjipEhGFI3e",
        "outputId": "4265bf40-5123-4d0b-f59f-a812e200b018"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@Karchary12: @realDonaldTrump @realDonaldTrump is the great be all the great be a great be and discastion the president the the country.\n",
            "@jestingers: @realDonaldTrump @realDonaldTrump @realDonaldTrump https://t.co/XE4I6IEFZ\n",
            "@MaryTerees: @realDonaldTrump @realDonaldTrump https://t.co/PWskFgzQi\n",
            "@Themittt: @realDonaldTrump @realDonaldTrump @realDonaldTrump https://t.co/7RRTVrsjET\n",
            "@marchannelly: @realDonaldTrump @realDonaldTrump is a great the proper the country and in the been a great better has been the the president\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "  print(sample_sequence_cuda(model, 140, 0.2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFFViGdoFb8E",
        "outputId": "111b2e84-a980-4d87-f511-c908d761ea4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "So Scotter live wall people and with the \"Edmbrigretely on the defeat. A really and fan the U.S. his an on all he to be and epenting on the \n",
            "@Anerginnam: @realDonaldTrump @realDonaldTrump The New Your propisting for are to you work for the interviewed by America!\n",
            "@AmlelBerice1: @realDonaldTrump I will be on @realDonaldTrump man on @FoxNews to respect and see president \"gonks the Pame over if you can m\n",
            "@Aralddritiem: @realDonaldTrump @realDonaldTrump #Trump2016 https://t.co/ywkRwzhpJ\n",
            "@Mirkotter: @realDonaldTrump Trump to the even so in the office to wo have pamed to $100000 very one Big Michighing not get histon the Unite\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "  print(sample_sequence_cuda(model, 140, 0.6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mxea8XeMFn0x",
        "outputId": "2e634390-9164-4e9b-8a12-1e295a02109c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@chivents: @realDonaldTrump @realDonaldTrump May You thanks.\n",
            "“Curat election is out of of this business same beeil ond back are campaills after the America will yestward to the alf.\n",
            "@MaFirtty20:  https://t.co/yMTpEFP8k\n",
            "I've won collase the promisles sisppon in MarketcKic  I winning of ObamaCare of made is one verabicagees don't totally millions and presiden\n",
            ".@DannyickNickets by Harsson and going to eucuss going was said (not us the remeckine days watch he debate to of the presibate! https://t.co\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "  print(sample_sequence_cuda(model, 140, 0.8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7NtY5IMFsAg",
        "outputId": "fc70002a-bf07-44bb-e116-1c51cf3b4060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I president way best thoud on Hellatueing Irubact! Today pasqual hes with up big emustle many laws we will ip! The people\n",
            "New a jobal craving” the enjoy!  intervicted mexiated. Je in Crookedro instway that a annmEn morated well menaiffor with the much can Interm\n",
            "@ScikePruin_Jor: See Presidential Marther's sealto win should back lijust never anfryrater8 that UN!\n",
            "The slanding crouccer Congratrover &amp; back im shourd alvoch our me lobring flopers shall: And BAC Sountred ar formed. They will be for ho\n",
            "Great - tell all when media &amp; vigetimate will best!\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "  print(sample_sequence_cuda(model, 140, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEP3-zBOFvSR",
        "outputId": "9168e8a5-60e8-4e09-f80c-8684107f8cf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@MtotManxe_pPE6Y  I: There w/ke totkey real minilue gnt JUNG #Pubulany Juslatumac Jone or eve #ure. Sove\n",
            "@EvineTfumpcqlavy Zight DonaldTruck iss.” Conglly.?👍! Sponoucled runnyines!#MA ot enjoyevoute” .M\"\n",
            "Only just”stom: 20ICE.YM.\n",
            "Avit/yetcoppctipost pacrs 4's. USSTAKW MAXA FRTIR! I lyaght-loving\n",
            ".@_S0Wez T❌RAFSVIRY JJC otwey Cuts I Donald Donsans a.CERFE PROCS.RC) Makm.\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "  print(sample_sequence_cuda(model, 140, 1.5))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Tut_6_Generative_RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}