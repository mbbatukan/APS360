{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Midterm Questions\n",
    "\n",
    "# Which of the following techniques does not help with vanishing gradient problem?\n",
    "\n",
    "1. ReLU non-linearity\n",
    "2. Auxiliary losses\n",
    "3. Skip-connections\n",
    "4. **Drop-out [1 Mark]**\n",
    "\n",
    "\n",
    "# Which one of the following is incorrect?\n",
    "\n",
    "1. Both convolutional and fully-connected networks are feed-forward.\n",
    "2. **There is one bias term per channel of a convolution kernel. [1 Mark]**\n",
    "3. Padding helps convolution to pay attention to border pixels as well.\n",
    "4. A convolutional layer is a fully-connected layer applied to neighboring patches of an image.\n",
    "\n",
    "\n",
    "# Which one of the following is not a regularization technique?\n",
    "\n",
    "1. Drop-out\n",
    "2. **Non-linear activation functions [1 Mark]**\n",
    "3. Adding some noise to gradients\n",
    "4. Weight decay\n",
    "\n",
    "\n",
    "# Suppose the output of a convolutional layer is a feature map of size 64x64 with depth of 128. If we apply global average pooling to this network, what is the output size?\n",
    "\n",
    "1. 64x64\n",
    "2. **128x1 [1 Mark]**\n",
    "3. 64x1\n",
    "4. 128x128\n",
    "\n",
    "\n",
    "# Which one of the following is incorrect about CNNs?\n",
    "\n",
    "1. The weight sharing occurs across all spatial dimensions.\n",
    "2. As we go deeper, the feature map resolution is decreased.\n",
    "3. As we go deeper, the dimension of channels is increased.\n",
    "4. **None of the above. [1 Mark]**\n",
    "\n",
    "\n",
    "# Which one of the following is incorrect about data augmentations?\n",
    "\n",
    "1. Augmentations preserve the semantic content.\n",
    "2. Augmentations help with generalization.\n",
    "3. Augmentations increase the data diversity.\n",
    "4. **Augmentations can change the class. [1 Mark]**\n",
    "\n",
    "\n",
    "# Which of the following is correct? \n",
    "\n",
    "1. In batch normalization, the summary statistics is computed for each sample independently.\n",
    "2. Batch normalization is compatible with stochastic gradient descent.\n",
    "3. We need moving averages at inference time for layer normalization.\n",
    "4. **Layer normalization is a non-parametric method. [1 Mark]**\n",
    "\n",
    "\n",
    "# Which one of the following is incorrect about optimizers?\n",
    "\n",
    "1. Adam optimizer uses a memory at least twice the number of parameters.\n",
    "2. Adam optimizer is able to handle the ravines in loss landscape.\n",
    "3. **Optimizers must be gradient-based. [1 Mark]**\n",
    "4. **SGD with momentum uses a memory at least twice the number of parameters. [1 Mark]**\n",
    "\n",
    "\n",
    "# Which one of the following is incorrect?\n",
    "\n",
    "1. The derivative of an output with respect to its skip connection is 1.\n",
    "2. Saddle points are more likely than local optima in loss landscape.\n",
    "3. In a fully-connected network, the layers before the final layer learn linearly-separable embeddings.\n",
    "4. **Kernels at the final layers of a convolutional network access more local information. [1 Mark]**\n",
    "\n",
    "# Which of the following is incorrect?\n",
    "\n",
    "1. Derivative of the loss function with respect to a weight indicates how much the weight is changing the loss.\n",
    "2. Derivative of the loss function with respect to a weight indicates if the weight is increasing or decreasing the loss.\n",
    "3. Derivative of the loss function with respect to an input feature indicates how much the network is paying attention to that feature.\n",
    "4. **Derivative of the loss function with respect to an input feature can help to train the network faster. [1 Mark]**\n",
    "\n",
    "\n",
    "# Suppose we want to design an image search engine where given a query image we retrieve top 20 images from our image dataset ordered by their similarity to the query image. Also suppose that we have access to a pretrained ResNet model. Explain in details how you would implement this search engine.\n",
    "\n",
    "1. Need to compute the embeddings for the query image, the image database, compute the similarity and sort by similarity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage \n",
    "\n",
    "# Example: 1-layer ANN with MSE and Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data (first column is the bias term)\n",
    "x = [[1, 0.1,-0.2], \n",
    "     [1,-0.1, 0.9], \n",
    "     [1, 1.2, 0.1], \n",
    "     [1, 1.1, 1.5]]\n",
    "# labels (desired output)\n",
    "t = [0, 0, 0, 1]\n",
    "# initial weights\n",
    "w = [1, -1, 1]\n",
    "\n",
    "iterations = 50\n",
    "learning = 10\n",
    "\n",
    "def simple_ann_MSE(x, w, t, iterations, learning):\n",
    "\n",
    "    E = []\n",
    "    \n",
    "    #iterate over epochs\n",
    "    for ii in range(iterations):\n",
    "        err = [] \n",
    "        y = []\n",
    "        #iterate over all the samples x\n",
    "        for n in range(len(x)):\n",
    "            v = 0\n",
    "            # compute w.x\n",
    "            for p in range(len(x[0])):\n",
    "                v = v + x[n][p]*w[p]\n",
    "            #sigmoidal activation    \n",
    "            y.append(1 / (1 + math.e**(-v))) \n",
    "            #MSE classification error\n",
    "            err.append((y[n]-t[n])**2)\n",
    "            #gradient descent to compute new weights\n",
    "            for p in range(len(w)):\n",
    "                d = x[n][p]*(y[n]-t[n])*(1-y[n])*(y[n])\n",
    "                w[p] = w[p] - learning*d\n",
    "                \n",
    "        #sum up classification error\n",
    "        E.append(sum(err)/len(x))\n",
    "    \n",
    "    return (y, w, E)\n",
    "\n",
    "(y, w, E) = simple_ann_MSE(x, w, t, iterations, learning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "# Example: 1-layer ANN with Cross-Entropy and Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_ann_CE(x, w, t, iterations, learning):\n",
    "    E = []\n",
    "    #iterate over epochs\n",
    "    for ii in range(iterations):\n",
    "        err = [] \n",
    "        y = []\n",
    "        #iterate over all the samples x\n",
    "        for n in range(len(x)):\n",
    "            v = 0\n",
    "            #compute w.x\n",
    "            for p in range(len(x[0])):\n",
    "                v = v + x[n][p]*w[p]\n",
    "            \n",
    "            #sigmoidal activation\n",
    "            y.append(1 / (1 + math.e**(-v))) \n",
    "            #cross-entropy classification error\n",
    "            err.append(-t[n]*math.log(y[n]+ 0.000001) - (1-t[n])*math.log(1-y[n]+ 0.000001))\n",
    "            #gradient descent to compute new weights\n",
    "            for p in range(len(w)):\n",
    "                d = x[n][p]*(y[n]-t[n]) #cross_entropy\n",
    "                w[p] = w[p] - learning*d\n",
    "        #sum up classification error\n",
    "        E.append(sum(err))\n",
    "    \n",
    "    return (y, w, E)\n",
    "\n",
    "(y, w, E) = simple_ann_CE(x, w, t, iterations, learning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "# Simple training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, batch_size=64, num_epochs=1 , print_stat = 1):\n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    iters, losses, train_acc, val_acc = [], [], [], []\n",
    "\n",
    "    # training\n",
    "    n = 0 # the number of iterations\n",
    "    for epoch in range(num_epochs):\n",
    "        for imgs, labels in iter(train_loader):\n",
    "            out = model(imgs)             # forward pass\n",
    "            loss = criterion(out, labels) # compute the total loss\n",
    "            loss.backward()               # backward pass (compute parameter updates)\n",
    "            optimizer.step()              # make the updates for each parameter\n",
    "            optimizer.zero_grad()         # a clean up step for PyTorch\n",
    "\n",
    "            # save the current training information\n",
    "            iters.append(n)\n",
    "            losses.append(float(loss)/batch_size)             # compute *average* loss\n",
    "            train_acc.append(get_accuracy(model, train=True)) # compute training accuracy \n",
    "            val_acc.append(get_accuracy(model, train=False))  # compute validation accuracy\n",
    "            n += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "# ANN vs CNN arhitectures for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANN_MNISTClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 50)\n",
    "        self.fc2 = nn.Linear(50, 20)\n",
    "        self.fc3 = nn.Linear(20, 10)  \n",
    "\n",
    "    def forward(self, img):\n",
    "        flattened = img.view(-1, 28 * 28)\n",
    "        activation1 = F.relu(self.fc1(flattened))\n",
    "        activation2 = F.relu(self.fc2(activation1))\n",
    "        output = self.fc3(activation2)\n",
    "        return output\n",
    "    \n",
    "    print('Artificial Neural Network Architecture (aka MLP) Done')\n",
    "\n",
    "#Convolutional Neural Network Architecture\n",
    "class CNN_MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_MNISTClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 5, 5) #in_channels, out_chanels, kernel_size\n",
    "        self.pool = nn.MaxPool2d(2, 2) #kernel_size, stride \n",
    "        self.conv2 = nn.Conv2d(5, 10, 5) #in_channels, out_chanels, kernel_size\n",
    "        self.fc1 = nn.Linear(160, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 160)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    print('Convolutional Neural Network Architecture Done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "# The general formula is this if you are interested: $[(W−K+2P)/S]+1$.\n",
    "\n",
    "* $W$ is the input size \n",
    "* $K$ is the Kernel size \n",
    "* $P$ is the padding \n",
    "* $S$ is the stride\n",
    "\n",
    "==============================================================\n",
    "\n",
    "* $28\\times 28$ (1ch) => conv1 => $24\\times24$ (5ch) --- (28-5+1)\n",
    "* $24\\times24$ (5ch) => pool => $12\\times12$ (5ch)\n",
    "* $12\\times12$ (5ch) => conv2 => $8\\times8$ (10ch)\n",
    "* $8\\times8$ (10ch) => pool => $4\\times4$ (10ch)\n",
    "* $4\\times4$ (10ch)  => Flat => $4\\times4\\times10 = 160$\n",
    "\n",
    "# Transfer learning (AlexNet)\n",
    "\n",
    "## Applying AlexNet on a Dataset\n",
    "In order to use transfer learning with AlexNet on a new dataset we will have to keep in mind how AlexNet was trained. AlexNet was trained on images of 3 x 224 x 224 images from the ImageNet dataset. These images are of higher resolution than what we have seen until now and are in colour. Hence, it would take significant effort to apply AlexNet to MNIST data, instead we will use another dataset.\n",
    "\n",
    "```python\n",
    "# confirm output from AlexNet feature extraction\n",
    "alexNet = torchvision.models.alexnet(pretrained=True)\n",
    "features = alexNet.features(images)\n",
    "features.shape\n",
    "```\n",
    "\n",
    "## Data Normalization\n",
    "\n",
    "*Data normalization* means to scale the input features of a neural\n",
    "network, so that all features are scaled similarly (similar means\n",
    "and standard deviations). Although data normalization does not directly\n",
    "prevent overfitting, normalizing your data makes the training\n",
    "problem easier.\n",
    "\n",
    "Data normalization is less of an issues for input data -- like images --\n",
    "where all input features have similar interpretations.\n",
    "All features of an image are pixel intensities, all of which are scaled\n",
    "the same way. However, if we were performing prediction of, say, \n",
    "housing prices based on a house's number of bedrooms, square footage, etc.,\n",
    "we would want each of the features to be scaled similarly. A scale\n",
    "of mean 0 and standard deviation 1 is one approach. Another approach\n",
    "is to scale each feature so that they are in the range `[0, 1]`.\n",
    "\n",
    "The PyTorch transform `transforms.ToTensor()` automatically scales\n",
    "each pixel intensity to the range `[0, 1]`.\n",
    "In your lab 2 code, we used the following transform:\n",
    "\n",
    "This transform subtracts 0.5 from each pixel, and divides the\n",
    "result by 0.5. So, each pixel intensity will be in the range `[-1, 1]`.\n",
    "In general, having both positive and negative input values helps\n",
    "the network trains quickly (because of the way weights are initialized).\n",
    "Sticking with each pixel being in the range `[0, 1]` is usually fine.\n",
    "\n",
    "## Data Augmentation\n",
    "\n",
    "While it is often expensive to gather more data, we can often\n",
    "programmatically *generate* more data points from our existing\n",
    "data set. We can make small alterations to our training set to obtain\n",
    "slightly different input data, but that is still valid.\n",
    "Common ways of obtaining new (image) data include:\n",
    "\n",
    "- Flipping each image horizontally or vertically (won't work for digit recognition, but might for other tasks)\n",
    "- Shifting each pixel a little to the left or right\n",
    "- Rotating the images a little\n",
    "- Adding noise to the image\n",
    "\n",
    "... or even a combination of the above. For demonstration purposes, let's randomly\n",
    "rotate our digits a little to get new training samples.\n",
    "\n",
    "\n",
    "## Weight Decay\n",
    "\n",
    "A more interesting technique that prevents overfitting is the idea of weight decay.\n",
    "The idea is to **penalize large weights**. We avoid large weights, because large weights\n",
    "mean that the prediction relies a lot on the content of one pixel, or on one unit. Intuitively,\n",
    "it does not make sense that the classification of an image should depend heavily on the \n",
    "content of one pixel, or even a few pixels.\n",
    "\n",
    "Mathematically, we penalize large weights by adding an extra term to the loss function,\n",
    "the term can look like the following:\n",
    "\n",
    "- $L^1$ regularization: $\\sum_k |w_k|$\n",
    "    - Mathematically, this term encourages weights to be exactly 0\n",
    "- $L^2$ regularization: $\\sum_k w_k^2$ \n",
    "    - Mathematically, in each iteration the weight is pushed towards 0\n",
    "- Combination of $L^1$ and $L^2$ regularization: add a term $\\sum_k |w_k| + w_k^2$  to the loss function.\n",
    "\n",
    "In PyTorch, weight decay can also be done automatically inside an optimizer. The parameter `weight_decay`\n",
    "of `optim.SGD` and most other optimizers uses $L^2$ regularization for weight decay. The value of the\n",
    "`weight_decay` parameter is another tunable hyperparameter.\n",
    "\n",
    "## Dropout\n",
    "\n",
    "Yet another way to prevent overfitting is to build **many** models, then average\n",
    "their predictions at test time. Each model might have a different set of\n",
    "initial weights.\n",
    "\n",
    "We won't show an example of model averaging here. Instead, we will show another \n",
    "idea that sounds drastically different on the surface.\n",
    "\n",
    "This idea is called **dropout**: we will randomly \"drop out\", \"zero out\", or \"remove\" a portion\n",
    "of neurons from each training iteration.\n",
    "\n",
    "In different iterations of training, we will drop out a different set of neurons.\n",
    "\n",
    "The technique has an effect of preventing weights from being overly dependent on\n",
    "each other: for example for one weight to be unnecessarily large to compensate for\n",
    "another unnecessarily large weight with the opposite sign. Weights are encouraged\n",
    "to be \"more independent\" of one another.\n",
    "\n",
    "During test time though, we will not drop out any neurons; instead we will use\n",
    "the entire set of weights. This means that our training time and test time behaviour\n",
    "of dropout layers are *different*. In the code for the function `train` and `get_accuracy`,\n",
    "we use `model.train()` and `model.eval()` to flag whether we want the model's training behaviour,\n",
    "or test time behaviour.\n",
    "\n",
    "While unintuitive, using all connections is a form\n",
    "of model averaging! We are effectively averaging over many different networks\n",
    "of various connectivity structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifierWithDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifierWithDropout, self).__init__()\n",
    "        self.layer1 = nn.Linear(28 * 28, 50)\n",
    "        self.layer2 = nn.Linear(50, 20)\n",
    "        self.layer3 = nn.Linear(20, 10)\n",
    "        self.dropout1 = nn.Dropout(0.4) # drop out layer with 20% dropped out neuron\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "    def forward(self, img):\n",
    "        flattened = img.view(-1, 28 * 28)\n",
    "        activation1 = F.relu(self.layer1(self.dropout1(flattened)))\n",
    "        activation2 = F.relu(self.layer2(self.dropout2(activation1)))\n",
    "        output = self.layer3(self.dropout3(activation2))\n",
    "        return output\n",
    "\n",
    "model = MNISTClassifierWithDropout()\n",
    "train(model, mnist_train, mnist_val, num_iters=500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        encoding_dim = 32\n",
    "        # encoder\n",
    "        self.fc1 = nn.Linear(28 * 28, encoding_dim)\n",
    "        # decoder\n",
    "        self.fc2 = nn.Linear(encoding_dim, 28*28)\n",
    "\n",
    "    def forward(self, img):\n",
    "        flattened = img.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(flattened))\n",
    "        # sigmoid for scaling output from 0 to 1\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "def train(model, num_epochs=5, batch_size=64, learning_rate=1e-3):\n",
    "    torch.manual_seed(42)\n",
    "    criterion = nn.MSELoss() # mean square error loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=learning_rate, \n",
    "                                 weight_decay=1e-5) # <--\n",
    "    train_loader = torch.utils.data.DataLoader(mnist_data, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True)\n",
    "    outputs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_loader:\n",
    "            img, _ = data\n",
    "            recon = model(img)\n",
    "            img = img.view(-1, 28 * 28)\n",
    "            loss = criterion(recon, img)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))\n",
    "        outputs.append((epoch, img, recon),)\n",
    "    return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "\n",
    "# Convolutional Autoencoder\n",
    "When working with image data it is often better to use a convolutional neural network and take advantage of the spatial relationships. The architecture for the encoder stage of a convolutional autoencoder will consist of standard convolutional layers that we have seen in our previous architectures. The decoder step will be a bit more tricky since we need a way to increase the resolution.  \n",
    "\n",
    "We need something akin to convolution, but that goes in the *opposite* direction. We will use something called a **transpose convolution**. Transpose convolutions were first called *deconvolutions*, since it is the ``inverse'' of a convolution operation. However, the terminology was confusing since it has nothing to do with the mathematical notion of deconvolution.\n",
    "\n",
    "## Implementation of a Convolutional Autoencoder\n",
    "\n",
    "To demonstrate the use of convolution transpose operations,\n",
    "we will build a **convolutional autoencoder**. Below is an example of a *convolutional* autoencoder that uses solely convolutional layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential( # like the Composition layer you built\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 7))\n",
    "            \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "def train(model, num_epochs=5, batch_size=64, learning_rate=1e-3):\n",
    "    torch.manual_seed(42)\n",
    "    criterion = nn.MSELoss() # mean square error loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=learning_rate, \n",
    "                                 weight_decay=1e-5)\n",
    "    train_loader = torch.utils.data.DataLoader(mnist_data, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True)\n",
    "    outputs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_loader:\n",
    "            img, _ = data\n",
    "            recon = model(img)\n",
    "            loss = criterion(recon, img)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))\n",
    "        outputs.append((epoch, img, recon),)\n",
    "    return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder\n",
    "\n",
    "To allow us to sample from the embedding space and generate new images, we add a constraint on the encoding network that forces it to generate latent vectors that roughly follow a unit Gaussian distribution. This constraint is what separates a variational autoencoder from the ones we've seen up until now.\n",
    "\n",
    "Now generating new images requires that we sample a latent vector from the unit Gaussian and pass it into the decoder.\n",
    "\n",
    "As shown in the figure, we will have encoding and decoding networks similar to what we used before, whether with fully-connected or convolutional layers. Then we add two additional linear layers to hold the mean and standard deviation vectors of the embedding space. We will need some way to generate a sampled latent space which will act as input to the decoding network.\n",
    "\n",
    "We will also need to update our loss function to use Kullback-Leibler divergence to constrain the embedding space to follow a unit Gaussian distribution. You will not be required to know the math behind this.\n",
    "\n",
    "A demonstration of the variational autoencoder is provided below.\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of latent space\n",
    "zdim = 25\n",
    "\n",
    "# Variational Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.fc1 = nn.Linear(28 * 28, 350)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2m = nn.Linear(350, zdim)  # mu layer\n",
    "        self.fc2s = nn.Linear(350, zdim)  # sd layer\n",
    "\n",
    "        # decoder\n",
    "        self.fc3 = nn.Linear(zdim, 350)\n",
    "        self.fc4 = nn.Linear(350, 28 * 28)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        return self.fc2m(h1), self.fc2s(h1)\n",
    "\n",
    "    # reparameterize\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = std.data.new(std.size()).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        return self.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 28 * 28))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "# Training Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function for VAE are unique and use Kullback-Leibler\n",
    "# divergence measure to force distribution to match unit Gaussian\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    bce = F.binary_cross_entropy(recon_x, x.view(-1, 28 * 28))\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    kld /= batch_size * 28 * 28\n",
    "    return bce + kld\n",
    "    \n",
    "def train(model, num_epochs = 1, batch_size = 64, learning_rate = 1e-3):\n",
    "    model.train() #train mode so that we do reparameterization\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(datasets.MNIST('data', \n",
    "               train=True, download=True, transform=transforms.ToTensor()),\n",
    "               batch_size = batch_size, shuffle = True)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "      for data in train_loader:  # load batch\n",
    "          img, _ = data\n",
    "          \n",
    "          recon, mu, logvar = model(img)\n",
    "          loss = loss_function(recon, img, mu, logvar) # calculate loss\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          optimizer.zero_grad()\n",
    "      \n",
    "      print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage \n",
    "# Recurrent Neural Networks\n",
    "\n",
    "## word2vec models\n",
    "\n",
    "A word2vec model learns embedding of words using the following architecture:\n",
    "\n",
    "- **Encoder**: word -> embedding\n",
    "- **Decoder**: embedding -> nearby words (context)\n",
    "\n",
    "Specific word2vec models differ in the which \"nearby words\" is predicted\n",
    "using the decoder: is it the 3 context words that appeared *before*\n",
    "the input word? Is it the 3 words that appeared *after*? Or is it a combination\n",
    "of the two words that appeared before and two words that appeared after\n",
    "the input word?\n",
    "\n",
    "These models are trained using a large corpus of text: for example the whole\n",
    "of Wikipedia or a large collection of news articles. We won't train our\n",
    "own word2vec models in this course, so we won't talk about the many considerations involved in training a word2vec model.\n",
    "\n",
    "Instead, we will use a set of pre-trained word embeddings. These are embeddings\n",
    "that someone else took the time and computational power to train.\n",
    "One of the most commonly-used pre-trained word embeddings are the **GloVe embeddings**.\n",
    "\n",
    "GloVe is a variation of a word2vec model. Again, the specifics of the algorithm\n",
    "and its training will be beyond the scope of this course.\n",
    "You should think of **GloVe embeddings** similarly to pre-trained AlexNet weights in that they \"may\" provide improvements to the representation of data.\n",
    "\n",
    "Unlike AlexNet, there are several variations of GloVe embeddings. They\n",
    "differ in the corpus used to train the embedding, and the *size* of the embeddings.\n",
    "\n",
    "## GloVe Embeddings\n",
    "\n",
    "To load pre-trained GloVe embeddings, we'll use a package called `torchtext`.\n",
    "The package `torchtext` contains other useful tools for working with text\n",
    "that we will see later in the course. \n",
    "\n",
    "We'll begin by loading a set of GloVe embeddings. The first time you run the code below, Python will download a large file (862MB) containing the pre-trained embeddings.\n",
    "\n",
    "It is a torch tensor with dimension `(50,)`. It is difficult to determine what each\n",
    "number in this embedding means, if anything. However, we know that there is structure\n",
    "in this embedding space. That is, distances in this embedding space is meaningful.\n",
    "\n",
    "## Measuring Distance\n",
    "\n",
    "To explore the structure of the embedding space, it is necessary to introduce\n",
    "a notion of *distance*. You are probably already familiar with the notion\n",
    "of the **Euclidean distance**. The Euclidean distance of two vectors $x = [x_1, x_2, ... x_n]$ and\n",
    "$y = [y_1, y_2, ... y_n]$ is just the 2-norm of their difference $x - y$. We can compute\n",
    "the Euclidean distance between $x$ and $y$:\n",
    "$\\sqrt{\\sum_i (x_i - y_i)^2}$\n",
    "\n",
    "\n",
    "An alternative measure of distance is the **Cosine Similarity**.\n",
    "The cosine similarity measures the *angle* between two vectors,\n",
    "and has the property that it only considers the *direction* of the\n",
    "vectors, not their magnitudes.\n",
    "\n",
    "## Analogies\n",
    "\n",
    "One surprising aspect of GloVe vectors is that the *directions* in the\n",
    "embedding space can be meaningful. The structure of the GloVe vectors\n",
    "certain analogy-like relationship like this tend to hold:\n",
    "\n",
    "$$ king - man + woman \\approx queen $$\n",
    "\n",
    "The $$doctor - man + woman \\approx nurse$$ analogy is very concerning.\n",
    "Just to verify, the same result does not appear if we flip the gender terms:\n",
    "\n",
    "## Sentiment analysis\n",
    "\n",
    "The columns we care about is the first one and the last one. The first column is the\n",
    "label (the label `0` means \"sad\" tweet, `4` means \"happy\" tweet), and the last column\n",
    "contains the tweet. Our task is to predict the sentiment of the tweet given the text.\n",
    "\n",
    "The approach today is as follows, for each tweet:\n",
    "\n",
    "1. We will split the text into words. We will do so by splitting at all whitespace\n",
    "   characters. There are better ways to perform the split, but let's keep our\n",
    "   dependencies light.\n",
    "2. We will look up the GloVe embedding of each word.\n",
    "   Words that do not have a GloVe vector will be ignored.\n",
    "3. We will sum up all the embeddings to get an embedding for an entire tweet.\n",
    "4. Finally, we will use a fully-connected neural network\n",
    "   to predict whether the tweet has positive or negative sentiment.\n",
    "\n",
    "First, let's sanity check that there are enough words for us to work with.\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = nn.Sequential(nn.Linear(50, 30),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(30, 10),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(10, 2))\n",
    "train_network(mymodel, train_loader, valid_loader, num_epochs=100, learning_rate=1e-4)\n",
    "\n",
    "def train_network(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    losses, train_acc, valid_acc = [], [], []\n",
    "    epochs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for tweets, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(tweets)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        losses.append(float(loss))     \n",
    "        if epoch % 5 == 4:\n",
    "            epochs.append(epoch)\n",
    "            train_acc.append(get_accuracy(model, train_loader))\n",
    "            valid_acc.append(get_accuracy(model, valid_loader))\n",
    "            print(\"Epoch %d; Loss %f; Train Acc %f; Val Acc %f\" % (\n",
    "                epoch+1, loss, train_acc[-1], valid_acc[-1]))\n",
    "\n",
    "def get_accuracy(model, data_loader):\n",
    "    correct, total = 0, 0\n",
    "    for tweets, labels in data_loader:\n",
    "        output = model(tweets)\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += labels.shape[0]\n",
    "    return correct / total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network Module\n",
    "\n",
    "PyTorch has variations of recurrent neural network modules.\n",
    "These modules computes the following:\n",
    "\n",
    "$$hidden = updatefn(hidden, input)$$\n",
    "$$output = outputfn(hidden)$$\n",
    "\n",
    "These modules are more complex and less intuitive than the usual\n",
    "neural network layers, so let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layer = nn.RNN(input_size=50,    # dimension of the input repr\n",
    "                   hidden_size=50,   # dimension of the hidden units\n",
    "                   batch_first=True) # input format is [batch_size, seq_len, repr_dim]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try and run this untrained `rnn_layer` on `tweet_emb`.\n",
    "We will need to add an extra dimension to `tweet_emb` to account for\n",
    "batching. We will also need to initialize a set of hidden units of size\n",
    "`[batch_size, 1, repr_dim]`, to be used for the *first* set of computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_input = tweet_emb.unsqueeze(0) # add the batch_size dimension\n",
    "h0 = torch.zeros(1, 1, 50)           # initial hidden state\n",
    "out, last_hidden = rnn_layer(tweet_input, h0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't technically have to explictly provide the initial hidden state,\n",
    "if we want to use an initial state of zeros. Just for today, we will be\n",
    "explicit about the hidden states that we provide.\n",
    "out2, last_hidden2 = rnn_layer(tweet_input)\n",
    "Now, let's look at the output and hidden dimensions that we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out.shape)\n",
    "print(last_hidden.shape)\n",
    "# torch.Size([1, 31, 50])\n",
    "# torch.Size([1, 1, 50])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "## RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(TweetRNN, self).__init__()\n",
    "        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Look up the embedding\n",
    "        x = self.emb(x)\n",
    "        # Set an initial hidden state\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        # Forward propagate the RNN\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = TweetRNN(50, 50, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we will need a different way of batching.\n",
    "\n",
    "One strategy is to **pad shorter sequences with zero inputs**, so that\n",
    "every sequence is the same length. The following PyTorch utilities\n",
    "are helpful.\n",
    "\n",
    "- `torch.nn.utils.rnn.pad_sequence`\n",
    "- `torch.nn.utils.rnn.pad_packed_sequence`\n",
    "- `torch.nn.utils.rnn.pack_sequence`\n",
    "- `torch.nn.utils.rnn.pack_padded_sequence`\n",
    "\n",
    "(Actually, there are more powerful helpers in the `torchtext` module\n",
    "that we will use in Lab 5. We'll stick to these in this demo, so that\n",
    "you can see what's actually going on under the hood.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "tweet_padded = pad_sequence([tweet for tweet, label in train[:10]],\n",
    "                            batch_first=True)\n",
    "print(tweet_padded.shape)\n",
    "print(tweet_padded[0:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "## TweetBatcher\n",
    "\n",
    "One issue we overlooked was that in our `TweetRNN` model, we always\n",
    "take the **last output unit** as input to the final classifier. Now\n",
    "that we are padding the input sequences, we should really be using\n",
    "the output at a previous time step. Recurrent neural networks therefore\n",
    "require much more record keeping than ANNs or even CNNs.\n",
    "\n",
    "There is yet another problem:\n",
    "the longest tweet has many, many more words than the shortest.\n",
    "Padding tweets so that every tweet has the same length as the longest\n",
    "tweet is impractical. Padding tweets in a mini-batch, however, is much\n",
    "more reasonable.\n",
    "\n",
    "In practice, practitioners will batch together tweets with the same\n",
    "length. For simplicity, we will do the same. We will implement a (more or less)\n",
    "straightforward way to batch tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class TweetBatcher:\n",
    "    def __init__(self, tweets, batch_size=32, drop_last=False):\n",
    "        # store tweets by length\n",
    "        self.tweets_by_length = {}\n",
    "        for words, label in tweets:\n",
    "            # compute the length of the tweet\n",
    "            wlen = words.shape[0]\n",
    "            # put the tweet in the correct key inside self.tweet_by_length\n",
    "            if wlen not in self.tweets_by_length:\n",
    "                self.tweets_by_length[wlen] = []\n",
    "            self.tweets_by_length[wlen].append((words, label),)\n",
    "         \n",
    "        #  create a DataLoader for each set of tweets of the same length\n",
    "        self.loaders = {wlen : torch.utils.data.DataLoader(\n",
    "                                    tweets,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=True,\n",
    "                                    drop_last=drop_last) # omit last batch if smaller than batch_size\n",
    "            for wlen, tweets in self.tweets_by_length.items()}\n",
    "        \n",
    "    def __iter__(self): # called by Python to create an iterator\n",
    "        # make an iterator for every tweet length\n",
    "        iters = [iter(loader) for loader in self.loaders.values()]\n",
    "        while iters:\n",
    "            # pick an iterator (a length)\n",
    "            im = random.choice(iters)\n",
    "            try:\n",
    "                yield next(im)\n",
    "            except StopIteration:\n",
    "                # no more elements in the iterator, remove it\n",
    "                iters.remove(im)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "## TweetLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(TweetLSTM, self).__init__()\n",
    "        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Look up the embedding\n",
    "        x = self.emb(x)\n",
    "        # Set an initial hidden state and cell state\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        # Forward propagate the LSTM\n",
    "        out, _ = self.rnn(x, (h0, c0))\n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = TweetLSTM(50, 50, 2)\n",
    "train_rnn_network(model, train_loader, valid_loader, num_epochs=20, learning_rate=2e-5)\n",
    "get_accuracy(model, test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "# Generative Recurrent Neural Networks (GNN)\n",
    "\n",
    "Last time we discussed using recurrent neural networks to make predictions about sequences. In particular, we treated tweets as a **sequence** of words. Since tweets can have a variable number of words, we needed an architecture that can take variable-sized sequences as input.\n",
    "\n",
    "This time, we will use recurrent neural networks to **generate** sequences.\n",
    "Generating sequences is more involved compared to making predictions about\n",
    "sequences. However, it is a very interesting task, and many students chose\n",
    "sequence-generation tasks for their projects.\n",
    "\n",
    "1. We need to be able to generate the *next* token, given the current \n",
    "   hidden state. In practice, we get a probability distribution over \n",
    "   the next token, and sample from that probability distribution.\n",
    "2. We need to be able to update the hidden state somehow. To do so,\n",
    "   we need two pieces of information: the old hidden state, and the actual\n",
    "   token that was generated in the previous step. The actual token generated\n",
    "   will inform the subsequent tokens.\n",
    "\n",
    "We will repeat both functions until a special \"END OF SEQUENCE\" token is\n",
    "generated.\n",
    "\n",
    "Note that there are several tricky things that we will have to figure out.\n",
    "For example, how do we actually sample the actual token from the probability\n",
    "distribution over tokens? What would we do during training, and how might \n",
    "that be different from during testing/evaluation? We will answer those\n",
    "questions as we implement the RNN.\n",
    "\n",
    "First, we will need to encode this tweet using a one-hot encoding.\n",
    "We'll build dictionary mappings\n",
    "from the character to the index of that character (a unique integer identifier),\n",
    "and from the index to the character. We'll use the same naming scheme that `torchtext`\n",
    "uses (`stoi` and `itos`).\n",
    "\n",
    "For simplicity, we'll work with a limited vocabulary containing\n",
    "just the characters in `tweet[100]`, plus two special tokens:\n",
    "\n",
    "- `<EOS>` represents \"End of String\", which we'll append to the end of our tweet.\n",
    "  Since tweets are variable-length, this is a way for the RNN to signal\n",
    "  that the entire sequence has been generated.\n",
    "- `<BOS>` represents \"Beginning of String\", which we'll prepend to the beginning of \n",
    "  our tweet. This is the first token that we will feed into the RNN.\n",
    "\n",
    "The way we use these special tokens will become more clear as we build the model.\n",
    "\n",
    "```python\n",
    "vocab = list(set(tweet)) + [\"<BOS>\", \"<EOS>\"]\n",
    "vocab_stoi = {s: i for i, s in enumerate(vocab)}\n",
    "vocab_itos = {i: s for i, s in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "## TextGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
    "        super(TextGenerator, self).__init__()\n",
    "\n",
    "        # identiy matrix for generating one-hot vectors\n",
    "        self.ident = torch.eye(vocab_size)\n",
    "\n",
    "        # recurrent neural network\n",
    "        self.rnn = nn.GRU(vocab_size, hidden_size, n_layers, batch_first=True)\n",
    "\n",
    "        # a fully-connect layer that outputs a distribution over\n",
    "        # the next token, given the RNN output\n",
    "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, inp, hidden=None):\n",
    "        inp = self.ident[inp]                  # generate one-hot vectors of input\n",
    "        output, hidden = self.rnn(inp, hidden) # get the next output and hidden state\n",
    "        output = self.decoder(output)          # predict distribution over next tokens\n",
    "        return output, hidden\n",
    "\n",
    "model = TextGenerator(vocab_size, 64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "## Training with Teacher Forcing\n",
    "\n",
    "At a very high level, we want our RNN model to have a high probability\n",
    "of generating the tweet. An RNN model generates text\n",
    "one character at a time based on the hidden state value.\n",
    "At each time step, we will check whether the model generated the\n",
    "correct character. That is, at each time step,\n",
    "we are trying to select the correct next character out of all the \n",
    "characters in our vocabulary. Recall that this problem is a multi-class\n",
    "classification problem, and we can use Cross-Entropy loss to train our\n",
    "network to become better at this type of problem.\n",
    "\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "```\n",
    "\n",
    "However, we don't just have a single multi-class classification problem.\n",
    "Instead, we have **one classification problem per time-step** (per token)!\n",
    "So, how do we predict the first token in the sequence? \n",
    "How do we predict the second token in the sequence? \n",
    "To help you understand what happens durign RNN training, we'll start with\n",
    "inefficient training code that shows you what happens step-by-step. We'll\n",
    "start with computing the loss for the first token generated, then the second token,\n",
    "and so on.\n",
    "So, let's start with the first classification problem: the problem of generating\n",
    "the **first** token (`tweet[0]`).\n",
    "To generate the first token, we'll feed the RNN network (with an initial, empty\n",
    "hidden state) the \"<BOS>\" token. Then, the output\n",
    "\n",
    "```python\n",
    "bos_input = torch.Tensor([vocab_stoi[\"<BOS>\"]])\n",
    "print(bos_input.shape, type(bos_input))\n",
    "bos_input = bos_input.long()\n",
    "print(bos_input.shape, type(bos_input))\n",
    "bos_input = bos_input.unsqueeze(0)\n",
    "print(bos_input.shape, type(bos_input))\n",
    "output, hidden = model(bos_input, hidden=None)\n",
    "output # distribution over the first token\n",
    "```\n",
    "\n",
    "We can compute the loss using `criterion`. Since the model is untrained,\n",
    "the loss is expected to be high. (For now, we won't do anything\n",
    "with this loss, and omit the backward pass.)\n",
    "\n",
    "```python\n",
    "target = torch.Tensor([vocab_stoi[tweet[0]]]).long().unsqueeze(0)\n",
    "criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
    "          target.reshape(-1))             # reshape to 1D tensor\n",
    "```\n",
    "\n",
    "Now, we need to update the hidden state and generate a prediction\n",
    "for the next token. To do so, we need to provide the current token to\n",
    "the RNN. We already said that during test time, we'll need to sample\n",
    "from the predicted probabilty over tokens that the neural network\n",
    "just generated. \n",
    "\n",
    "Right now, we can do something better: we can **use the ground-truth,\n",
    "actual target token**. This technique is called **teacher-forcing**, \n",
    "and generally speeds up training. The reason is that right now, \n",
    "since our model does not perform well, the predicted probability\n",
    "distribution is pretty far from the ground truth. So, it is very,\n",
    "very difficult for the neural network to get back on track given bad\n",
    "input data.\n",
    "\n",
    "```python\n",
    "# Use teacher-forcing: we pass in the ground truth `target`,\n",
    "# rather than using the NN predicted distribution\n",
    "output, hidden = model(target, hidden)\n",
    "output # distribution over the second token\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "\n",
    "Finally, with our final token, we should expect to output the \"<EOS>\"\n",
    "token, so that our RNN learns when to stop generating characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hidden = model(target, hidden)\n",
    "target = torch.Tensor([vocab_stoi[\"<EOS>\"]]).long().unsqueeze(0)\n",
    "loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
    "                 target.reshape(-1))             # reshape to 1D tensor\n",
    "print(i, output, loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we don't really need a loop. Recall that in a predictive RNN,\n",
    "the `nn.RNN` module can take an entire sequence as input. We can do the\n",
    "same thing here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_ch = [\"<BOS>\"] + list(tweet) + [\"<EOS>\"]\n",
    "tweet_indices = [vocab_stoi[ch] for ch in tweet_ch]\n",
    "tweet_tensor = torch.Tensor(tweet_indices).long().unsqueeze(0)\n",
    "\n",
    "print(tweet_tensor.shape)\n",
    "\n",
    "output, hidden = model(tweet_tensor[:,:-1]) # <EOS> is never an input token\n",
    "target = tweet_tensor[:,1:]                 # <BOS> is never a target token\n",
    "loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
    "                 target.reshape(-1))             # reshape to 1D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for it in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    output, _ = model(tweet_tensor[:,:-1])\n",
    "    loss = criterion(output.reshape(-1, vocab_size),\n",
    "                 target.reshape(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (it+1) % 100 == 0:\n",
    "        print(\"[Iter %d] Loss %f\" % (it+1, float(loss)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "## Generating a Token\n",
    "\n",
    "At this point, we want to see whether our model is actually learning\n",
    "something. So, we need to talk about how to\n",
    "actually use the RNN model to generate text. If we can \n",
    "generate text, we can make a qualitative asssessment of how well\n",
    "our RNN is performing.\n",
    "\n",
    "The main difference between training and test-time (generation time)\n",
    "is that we don't have the ground-truth tokens to feed as inputs\n",
    "to the RNN. Instead, we need to actually **sample** a token based\n",
    "on the neural network's prediction distribution.\n",
    "\n",
    "But how can we sample a token from a distribution?\n",
    "\n",
    "On one extreme, we can always take\n",
    "the token with the largest probability (argmax). This has been our\n",
    "go-to technique in other classification tasks. However, this idea\n",
    "will fail here. The reason is that in practice, \n",
    "**we want to be able to generate a variety of different sequences from\n",
    "the same model**. An RNN that can only generate a single new Trump Tweet\n",
    "is fairly useless.\n",
    "\n",
    "In short, we want some randomness. We can do so by using the logit\n",
    "outputs from our model to construct a multinomial distribution over\n",
    "the tokens and then sample a random token from that multinomial distribution.\n",
    "\n",
    "One natural multinomial distribution we can choose is the \n",
    "distribution we get after applying the softmax on the outputs.\n",
    "However, we will do one more thing: we will add a **temperature**\n",
    "parameter to manipulate the softmax outputs. We can set a\n",
    "**higher temperature** to make the probability of each token\n",
    "**more even** (more random), or a **lower temperature** to assign\n",
    "more probability to the tokens with a higher logit (output).\n",
    "A **higher temperature** means that we will get a more diverse sample,\n",
    "with potentially more mistakes. A **lower temperature** means that we\n",
    "may see repetitions of the same high probability sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence(model, max_len=100, temperature=0.8):\n",
    "    generated_sequence = \"\"\n",
    "   \n",
    "    inp = torch.Tensor([vocab_stoi[\"<BOS>\"]]).long()\n",
    "    hidden = None\n",
    "    for p in range(max_len):\n",
    "        output, hidden = model(inp.unsqueeze(0), hidden)\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = vocab_itos[top_i]\n",
    "        \n",
    "        if predicted_char == \"<EOS>\":\n",
    "            break\n",
    "        generated_sequence += predicted_char       \n",
    "        inp = torch.Tensor([top_i]).long()\n",
    "    return generated_sequence\n",
    "\n",
    "print(sample_sequence(model, temperature=0.8))\n",
    "print(sample_sequence(model, temperature=1.0))\n",
    "print(sample_sequence(model, temperature=1.5))\n",
    "print(sample_sequence(model, temperature=2.0))\n",
    "print(sample_sequence(model, temperature=5.0))\n",
    "\n",
    "# God Bless the people of Venezuela!\n",
    "# God Bless the people of Venezuela!\n",
    "# God Blesstthh peoplefof VenezuelalaG\n",
    "# GolsBless the people of VenezfeuVa\n",
    "# h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative RNN using GPU\n",
    "Training a generative RNN can be a slow process. Here's a sample GPU implementation to speed up the training. The changes required to enable GPU are provided in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence_cuda(model, max_len=100, temperature=0.8):\n",
    "    generated_sequence = \"\"\n",
    "   \n",
    "    inp = torch.Tensor([vocab_stoi[\"<BOS>\"]]).long().cuda()    # <----- GPU\n",
    "    hidden = None\n",
    "    for p in range(max_len):\n",
    "        output, hidden = model(inp.unsqueeze(0), hidden)\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp().cpu()\n",
    "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = vocab_itos[top_i]\n",
    "\n",
    "        if predicted_char == \"<EOS>\":\n",
    "            break\n",
    "        generated_sequence += predicted_char       \n",
    "        inp = torch.Tensor([top_i]).long().cuda()    # <----- GPU\n",
    "    return generated_sequence\n",
    "\n",
    "\n",
    "def train_cuda(model, data, batch_size=1, num_epochs=1, lr=0.001, print_every=100):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    it = 0\n",
    "    data_iter = torchtext.legacy.data.BucketIterator(data,\n",
    "                                              batch_size=batch_size,\n",
    "                                              sort_key=lambda x: len(x.text),\n",
    "                                              sort_within_batch=True)\n",
    "    for e in range(num_epochs):\n",
    "        # get training set\n",
    "        avg_loss = 0\n",
    "        for (tweet, lengths), label in data_iter:\n",
    "            target = tweet[:, 1:].cuda()              # <------- GPU\n",
    "            inp = tweet[:, :-1].cuda()                # <------- GPU\n",
    "            # cleanup\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            output, _ = model(inp)\n",
    "            loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1))\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss\n",
    "            it += 1 # increment iteration count\n",
    "            if it % print_every == 0:\n",
    "                print(\"[Iter %d] Loss %f\" % (it+1, float(avg_loss/print_every)))\n",
    "                print(\"    \" + sample_sequence_cuda(model, 140, 0.8))\n",
    "                avg_loss = 0\n",
    "\n",
    "model = TextGenerator(vocab_size, 64)\n",
    "model = model.cuda()\n",
    "model.ident = model.ident.cuda()\n",
    "train_cuda(model, trump_tweets, batch_size=32, num_epochs=1, lr=0.004, print_every=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "# Tutorial 7a - Generative Adversarial Networks\n",
    "\n",
    "Thus far, we have discussed several **generative** models. A generative\n",
    "model learns the *structure* of a set of input data. In doing so, the model\n",
    "learns to *generate* new data that it has never seen before in the training\n",
    "data. The generative models we discussed were:\n",
    "\n",
    "- an autoencoder\n",
    "- an RNN used to generate text\n",
    "\n",
    "A Generative Adversarial Network (GAN) is yet another example of a generative\n",
    "model. To motivate the GAN, let's first discuss the drawbacks of an autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28) # flatten image\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        out = self.fc4(x)\n",
    "        return out\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(100, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 128)\n",
    "        self.fc4 = nn.Linear(128, 28*28)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        out = F.tanh(self.fc4(x))\n",
    "        return out\n",
    "\n",
    "D = Discriminator()\n",
    "G = Generator()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, both the Discriminator and Generator are fully-connected networks.\n",
    "One difference between these models and the previous models we've built is\n",
    "that we are using a `nn.LeakyReLU` activation.\n",
    "\n",
    "Leaky ReLU is a variation\n",
    "of the ReLU activation that lets some information through, even when\n",
    "its input is less than 0. The layer `nn.LeakyReLU(0.2)` \n",
    "performs the computation: `x if x > 0 else 0.2 * x`.\n",
    "\n",
    "But what loss function should we optimize? Consider the following quantity:\n",
    "\n",
    "`P(D correctly identifies real image) + P(D correctly identifies image generated by G)`\n",
    "\n",
    "A good **discriminator** would want to *maximize* the above quanity by altering\n",
    "its parameters.\n",
    "\n",
    "Likewise, a good **generator** would want to *minimize* the above quanity. Actually,\n",
    "the only term that the generator controls is `P(D correctly identifies image generated by G)`\n",
    "So, the best thing for the generator to do is alter its parameters to generate images\n",
    "that can fool D.\n",
    "\n",
    "Since we are looking at class probabilities, we will use binary cross entropy loss.\n",
    "\n",
    "Here is a training loop to train a GAN.\n",
    "For every minibatch of data, we train the discriminator for one iteration,\n",
    "and then we train the generator for one iteration.\n",
    "\n",
    "For the discriminator, we use the label `0` to represent a **fake** image, and `1` to represent\n",
    "a real image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(G, D, lr=0.002, batch_size=64, num_epochs=20):\n",
    "\n",
    "    rand_size = 100;\n",
    "\n",
    "    # optimizers for generator and discriminator\n",
    "    d_optimizer = optim.Adam(D.parameters(), lr)\n",
    "    g_optimizer = optim.Adam(G.parameters(), lr)\n",
    " \n",
    "    # define loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # get the training datasets\n",
    "    train_data = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    # prepare data loader\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # keep track of loss and generated, \"fake\" samples\n",
    "    samples = []\n",
    "    losses = []\n",
    "\n",
    "    # fixed data for testing\n",
    "    sample_size=16\n",
    "    test_noise = np.random.uniform(-1, 1, size=(sample_size, rand_size))\n",
    "    test_noise = torch.from_numpy(test_noise).float()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        D.train()\n",
    "        G.train()\n",
    "        \n",
    "        for batch_i, (real_images, _) in enumerate(train_loader):\n",
    "                    \n",
    "            batch_size = real_images.size(0)\n",
    "            \n",
    "            # rescale images to range -1 to 1\n",
    "            real_images = real_images*2 - 1\n",
    "            \n",
    "            # === Train the Discriminator ===\n",
    "            \n",
    "            d_optimizer.zero_grad()\n",
    "\n",
    "            # discriminator losses on real images \n",
    "            D_real = D(real_images)\n",
    "            labels = torch.ones(batch_size)\n",
    "            d_real_loss = criterion(D_real.squeeze(), labels)\n",
    "            \n",
    "            # discriminator losses on fake images\n",
    "            z = np.random.uniform(-1, 1, size=(batch_size, rand_size))\n",
    "            z = torch.from_numpy(z).float()\n",
    "            fake_images = G(z)\n",
    "\n",
    "            D_fake = D(fake_images)\n",
    "            labels = torch.zeros(batch_size) # fake labels = 0\n",
    "            d_fake_loss = criterion(D_fake.squeeze(), labels)\n",
    "            \n",
    "            # add up losses and update parameters\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "\n",
    "            # === Train the Generator ===\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            # generator losses on fake images\n",
    "            z = np.random.uniform(-1, 1, size=(batch_size, rand_size))\n",
    "            z = torch.from_numpy(z).float()\n",
    "            fake_images = G(z)\n",
    "          \n",
    "            D_fake = D(fake_images)\n",
    "            labels = torch.ones(batch_size) #flipped labels\n",
    "\n",
    "            # compute loss and update parameters\n",
    "            g_loss = criterion(D_fake.squeeze(), labels)\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "        # print loss\n",
    "        print('Epoch [%d/%d], d_loss: %.4f, g_loss: %.4f, ' \n",
    "              % (epoch + 1, num_epochs, d_loss.item(), g_loss.item()))\n",
    "\n",
    "        # append discriminator loss and generator loss\n",
    "        losses.append((d_loss.item(), g_loss.item()))\n",
    "        \n",
    "        # plot images\n",
    "        G.eval()\n",
    "        D.eval()\n",
    "        test_images = G(test_noise)\n",
    "\n",
    "        plt.figure(figsize=(9, 3))\n",
    "        for k in range(16):\n",
    "            plt.subplot(2, 8, k+1)\n",
    "            plt.imshow(test_images[k,:].data.numpy().reshape(28, 28), cmap='Greys')\n",
    "        plt.show()\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "GANs are notoriously difficult to train. One difficulty is that a training curve is no longer\n",
    "as helpful as it was for a supervised learning problem! The generator and discriminator losses\n",
    "tend to bounce up and down, since both the generator and discriminator are changing over time.\n",
    "Tuning hyperparameters is also much more difficult because we don't have the training curve\n",
    "to guide us. Newer GAN models like Wasserstein GAN tries to alleviate some of these issues, but\n",
    "are beyond the scope of this course.\n",
    "\n",
    "To compound the difficulty of hyperparameter tuning, GANs also take a long time to train.\n",
    "It is tempting to stop training early, but the effects of hyperparameters may not be noticable\n",
    "until later on in training.\n",
    "\n",
    "You might have noticed in the images generated by our simple GAN that the model seems to only\n",
    "output a small number of digit types. This phenomenon is called **mode collapse**. A \n",
    "generator can optimize `P(D correctly identifies image generated by G)` by learning\n",
    "to generate one type of input (e.g. one digit) really well, and not learning how to\n",
    "generate any other digits at all!\n",
    "\n",
    "To prevent mode collapse, newer variations of GANs provides the discriminator\n",
    "with a *small set* of either real or fake data, rather than one at a time. A discriminator\n",
    "would therefore be able to use the variety of the generated data as a feature to\n",
    "determine whether the entire small set of data is real or fake.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "# Adversarial Examples\n",
    "\n",
    "This notebook demonstrates how easy it is to create adversarial examples.\n",
    "Let's start by training some models to classify the digits in the\n",
    "MNIST data set. We'll work with one fully-connected neural network\n",
    "and one convolutional network to show the generality of our approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(28 * 28, 50)\n",
    "        self.layer2 = nn.Linear(50, 20)\n",
    "        self.layer3 = nn.Linear(20, 10)\n",
    "    def forward(self, img):\n",
    "        flattened = img.view(-1, 28 * 28)\n",
    "        activation1 = F.relu(self.layer1(flattened))\n",
    "        activation2 = F.relu(self.layer2(activation1))\n",
    "        output = self.layer3(activation2)\n",
    "        return output\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 5, 5, padding=2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(5, 10, 5, padding=2)\n",
    "        self.fc1 = nn.Linear(10 * 7 * 7, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 10 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.squeeze(1)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, batch_size=64, lr=0.001, num_iters=1000, print_every=100): \n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0\n",
    "    n = 0\n",
    "\n",
    "    while True:\n",
    "        for imgs, labels in iter(train_loader):\n",
    "            out = model(imgs)\n",
    "            loss = criterion(out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "            n += 1\n",
    "        \n",
    "            if n % print_every == 0:\n",
    "                print(\"Iter %d. Avg.Loss: %f\" % (n, total_loss/print_every))\n",
    "                total_loss = 0\n",
    "            if n > num_iters:\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_model = FCNet()\n",
    "train(fc_model, mnist_images, num_iters=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "## Targetted Adversarial Attack\n",
    "\n",
    "The purpose of an adversarial attack is to perturb an input \n",
    "(usually an image $x$) so that a neural network $f$ misclassifies\n",
    "the perturbed image $x + \\epsilon$. In a targeted attack, we\n",
    "want the network $f$ to misclassify the perturbed image into\n",
    "a class of our choosing.\n",
    "\n",
    "Let's begin with this image. We will perturb the image so our model\n",
    "thinks that the image is of the digit 3, when in fact it is of the\n",
    "digit 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(1, 28, 28) * 0.01\n",
    "noise.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([noise], lr=0.01, weight_decay=1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(1000):\n",
    "    adv_image = torch.clamp(image + noise, 0, 1)\n",
    "    out = model(adv_image.unsqueeze(0))\n",
    "    loss = criterion(out, torch.Tensor([target_label]).long())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adversarial_example(model, image, target_label):\n",
    "    noise = torch.randn(1, 28, 28)\n",
    "    noise.requires_grad = True\n",
    "    \n",
    "    optimizer = optim.Adam([noise], lr=0.01, weight_decay=1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for i in range(1000):\n",
    "        adv_image = torch.clamp(image + noise, 0, 1)\n",
    "        out = model(adv_image.unsqueeze(0))\n",
    "        loss = criterion(out, torch.Tensor([target_label]).long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
